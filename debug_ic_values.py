#!/usr/bin/env python3
"""
ğŸ” IC ê°’ ìƒì„¸ ê²€ì¦ - ë°ì´í„° ëˆ„ìˆ˜ ë° ê³„ì‚° ì˜¤ë¥˜ í™•ì¸
"""

import pandas as pd
import numpy as np
from scipy.stats import spearmanr, pearsonr
import json
from pathlib import Path

def load_actual_predictions():
    """ì‹¤ì œ ì˜ˆì¸¡ ê²°ê³¼ ë¡œë“œ"""
    
    print("ğŸ” ì‹¤ì œ ì˜ˆì¸¡ ê²°ê³¼ ë¶„ì„...")
    
    # í‰ê°€ ê²°ê³¼ íŒŒì¼ ì°¾ê¸°
    eval_files = list(Path('models/price_prediction/predictions').glob('*.json'))
    if not eval_files:
        print("âŒ í‰ê°€ ê²°ê³¼ íŒŒì¼ ì—†ìŒ")
        return
    
    latest_file = sorted(eval_files)[-1]
    print(f"ğŸ“ íŒŒì¼: {latest_file}")
    
    with open(latest_file, 'r') as f:
        data = json.load(f)
    
    # ê° ëª¨ë¸ë³„ ë¶„ì„
    for model_name in ['ar', 'price_lgbm', 'reddit_lgbm']:
        if model_name not in data['predictions']:
            continue
            
        print(f"\nğŸ” {model_name.upper()} ëª¨ë¸ ë¶„ì„:")
        
        predictions = data['predictions'][model_name]
        
        # ì²« ë²ˆì§¸ splitë§Œ ë¶„ì„
        if len(predictions) > 0:
            split_data = predictions[0]  # ì²« ë²ˆì§¸ split
            
            # ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ ì¶”ì¶œ
            y_pred = [item['y_pred'] for item in split_data]
            y_true = [item['y1d'] for item in split_data]
            
            y_pred = np.array(y_pred)
            y_true = np.array(y_true)
            
            print(f"   ìƒ˜í”Œ ìˆ˜: {len(y_pred)}")
            print(f"   ì˜ˆì¸¡ê°’ ë²”ìœ„: [{y_pred.min():.6f}, {y_pred.max():.6f}]")
            print(f"   ì‹¤ì œê°’ ë²”ìœ„: [{y_true.min():.6f}, {y_true.max():.6f}]")
            print(f"   ì˜ˆì¸¡ê°’ í‘œì¤€í¸ì°¨: {y_pred.std():.6f}")
            print(f"   ì‹¤ì œê°’ í‘œì¤€í¸ì°¨: {y_true.std():.6f}")
            
            # IC ì¬ê³„ì‚°
            if len(y_pred) > 2:
                pearson_ic, pearson_p = pearsonr(y_pred, y_true)
                spearman_ic, spearman_p = spearmanr(y_pred, y_true)
                
                print(f"   ğŸ“ˆ Pearson IC: {pearson_ic:.6f} (p={pearson_p:.6f})")
                print(f"   ğŸ“ˆ Spearman IC: {spearman_ic:.6f} (p={spearman_p:.6f})")
                
                # ì˜ì‹¬ìŠ¤ëŸ¬ìš´ íŒ¨í„´ í™•ì¸
                check_suspicious_patterns(y_pred, y_true, model_name)
            
            # ì²« 10ê°œ ìƒ˜í”Œ í™•ì¸
            print(f"   ğŸ“Š ì²« 10ê°œ ìƒ˜í”Œ:")
            for i in range(min(10, len(y_pred))):
                print(f"      {i+1}: pred={y_pred[i]:.6f}, true={y_true[i]:.6f}, diff={abs(y_pred[i]-y_true[i]):.6f}")

def check_suspicious_patterns(y_pred, y_true, model_name):
    """ì˜ì‹¬ìŠ¤ëŸ¬ìš´ íŒ¨í„´ í™•ì¸"""
    
    # 1. ë™ì¼í•œ ê°’ë“¤ í™•ì¸
    identical_ratio = np.mean(np.abs(y_pred - y_true) < 1e-8)
    if identical_ratio > 0.01:  # 1% ì´ìƒì´ ê±°ì˜ ë™ì¼
        print(f"   ğŸš¨ {identical_ratio:.1%}ì˜ ì˜ˆì¸¡ê°’ì´ ì‹¤ì œê°’ê³¼ ê±°ì˜ ë™ì¼!")
    
    # 2. ìƒìˆ˜ ì˜ˆì¸¡ í™•ì¸
    pred_variance = np.var(y_pred)
    if pred_variance < 1e-8:
        print(f"   ğŸš¨ ëª¨ë“  ì˜ˆì¸¡ê°’ì´ ê±°ì˜ ë™ì¼ (ë¶„ì‚°: {pred_variance:.2e})")
    
    # 3. ê·¹ë‹¨ì  ìƒê´€ê´€ê³„ í™•ì¸
    corr = np.corrcoef(y_pred, y_true)[0, 1]
    if abs(corr) > 0.8:
        print(f"   ğŸš¨ ê·¹ë‹¨ì ìœ¼ë¡œ ë†’ì€ ìƒê´€ê´€ê³„: {corr:.6f}")
        
        # ì‚°ì ë„ íŒ¨í„´ í™•ì¸
        if corr > 0.95:
            print("   ğŸš¨ ê±°ì˜ ì™„ë²½í•œ ì–‘ì˜ ìƒê´€ê´€ê³„ - ë°ì´í„° ëˆ„ìˆ˜ ì˜ì‹¬!")
    
    # 4. ë¯¸ë˜ ì •ë³´ ì‚¬ìš© ì˜ì‹¬
    # ì˜ˆì¸¡ê°’ì´ ì‹¤ì œê°’ì„ ë„ˆë¬´ ì˜ ë§ì¶”ëŠ” ê²½ìš°
    mae = np.mean(np.abs(y_pred - y_true))
    std_true = np.std(y_true)
    
    if mae < std_true * 0.1:  # MAEê°€ í‘œì¤€í¸ì°¨ì˜ 10% ë¯¸ë§Œ
        print(f"   ğŸš¨ ì˜ˆì¸¡ ì˜¤ì°¨ê°€ ë„ˆë¬´ ì‘ìŒ: MAE={mae:.6f}, STD={std_true:.6f}")
        print("   ğŸš¨ ë¯¸ë˜ ì •ë³´ ì‚¬ìš© ì˜ì‹¬!")

def check_data_splits():
    """ë°ì´í„° ë¶„í•  ê²€ì¦"""
    
    print("\nğŸ• ë°ì´í„° ë¶„í•  ê²€ì¦...")
    
    try:
        # ë°ì´í„°ì…‹ ë¡œë“œ
        train_df = pd.read_csv('data/colab_datasets/tabular_train_20250814_031335.csv')
        val_df = pd.read_csv('data/colab_datasets/tabular_val_20250814_031335.csv')  
        test_df = pd.read_csv('data/colab_datasets/tabular_test_20250814_031335.csv')
        
        # ë‚ ì§œ ë³€í™˜
        for df in [train_df, val_df, test_df]:
            df['date'] = pd.to_datetime(df['date'])
        
        print(f"ğŸ“Š Train: {train_df['date'].min()} ~ {train_df['date'].max()}")
        print(f"ğŸ“Š Val: {val_df['date'].min()} ~ {val_df['date'].max()}")
        print(f"ğŸ“Š Test: {test_df['date'].min()} ~ {test_df['date'].max()}")
        
        # ê²¹ì¹¨ í™•ì¸
        train_dates = set(train_df['date'])
        val_dates = set(val_df['date'])
        test_dates = set(test_df['date'])
        
        overlaps = [
            ('Train-Val', len(train_dates & val_dates)),
            ('Val-Test', len(val_dates & test_dates)),
            ('Train-Test', len(train_dates & test_dates))
        ]
        
        for name, count in overlaps:
            if count > 0:
                print(f"ğŸš¨ {name} ê²¹ì¹¨: {count}ê°œ")
            else:
                print(f"âœ… {name} ê²¹ì¹¨ ì—†ìŒ")
                
        # ì‹œê°„ ìˆœì„œ í™•ì¸
        if train_df['date'].max() >= val_df['date'].min():
            print(f"ğŸš¨ Train ë°ì´í„°ê°€ Val ë°ì´í„°ì™€ ì‹œê°„ì ìœ¼ë¡œ ê²¹ì¹¨!")
        if val_df['date'].max() >= test_df['date'].min():
            print(f"ğŸš¨ Val ë°ì´í„°ê°€ Test ë°ì´í„°ì™€ ì‹œê°„ì ìœ¼ë¡œ ê²¹ì¹¨!")
            
    except Exception as e:
        print(f"âŒ ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {e}")

def analyze_feature_target_correlation():
    """íŠ¹ì„±-íƒ€ê²Ÿ ìƒê´€ê´€ê³„ ë¶„ì„"""
    
    print("\nğŸ” íŠ¹ì„±-íƒ€ê²Ÿ ìƒê´€ê´€ê³„ ë¶„ì„...")
    
    try:
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
        test_df = pd.read_csv('data/colab_datasets/tabular_test_20250814_031335.csv')
        
        # íŠ¹ì„± ì»¬ëŸ¼ (íƒ€ê²Ÿ ì œì™¸)
        exclude_cols = ['date', 'ticker', 'ticker_type', 'y1d', 'y5d', 
                       'alpha_1d', 'alpha_5d', 'direction_1d', 'direction_5d']
        feature_cols = [col for col in test_df.columns if col not in exclude_cols]
        
        # ìƒê´€ê´€ê³„ ê³„ì‚°
        correlations = []
        for col in feature_cols:
            if pd.api.types.is_numeric_dtype(test_df[col]):
                corr = test_df[col].corr(test_df['y1d'])
                if not np.isnan(corr):
                    correlations.append((col, abs(corr)))
        
        # ìƒìœ„ 10ê°œ íŠ¹ì„±
        correlations.sort(key=lambda x: x[1], reverse=True)
        
        print("ğŸ“Š íƒ€ê²Ÿê³¼ ìƒê´€ê´€ê³„ê°€ ë†’ì€ íŠ¹ì„±ë“¤:")
        for i, (col, corr) in enumerate(correlations[:10]):
            print(f"   {i+1:2d}. {col}: {corr:.4f}")
            
            # ì˜ì‹¬ìŠ¤ëŸ½ê²Œ ë†’ì€ ìƒê´€ê´€ê³„
            if corr > 0.8:
                print(f"       ğŸš¨ ë§¤ìš° ë†’ì€ ìƒê´€ê´€ê³„! ë°ì´í„° ëˆ„ìˆ˜ ì˜ì‹¬")
                
    except Exception as e:
        print(f"âŒ íŠ¹ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    
    print("ğŸ” IC ê°’ ìƒì„¸ ê²€ì¦ ë° ë°ì´í„° ëˆ„ìˆ˜ í™•ì¸")
    print("=" * 60)
    
    # 1. ì‹¤ì œ ì˜ˆì¸¡ ê²°ê³¼ ë¶„ì„
    load_actual_predictions()
    
    # 2. ë°ì´í„° ë¶„í•  ê²€ì¦
    check_data_splits()
    
    # 3. íŠ¹ì„±-íƒ€ê²Ÿ ìƒê´€ê´€ê³„ ë¶„ì„
    analyze_feature_target_correlation()
    
    print("\n" + "=" * 60)
    print("ğŸ“‹ ê²€ì¦ ìš”ì•½:")
    print("   1. IC ê°’ì´ 0.08ì€ ë§¤ìš° ë†’ì€ ìˆ˜ì¤€")
    print("   2. ì¼ë°˜ì ìœ¼ë¡œ ê¸ˆìœµ ë°ì´í„°ì—ì„œ IC > 0.05ëŠ” ì˜ì‹¬ìŠ¤ëŸ¬ì›€")
    print("   3. ë°ì´í„° ëˆ„ìˆ˜ë‚˜ ë¯¸ë˜ ì •ë³´ ì‚¬ìš© ê°€ëŠ¥ì„± í™•ì¸ í•„ìš”")
    print("   4. ì‹œê³„ì—´ ë°ì´í„°ì˜ ì •í™•í•œ ë¶„í•  í™•ì¸ í•„ìš”")

if __name__ == "__main__":
    main()