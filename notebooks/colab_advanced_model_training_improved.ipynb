{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# üöÄ **Advanced Meme Stock Prediction Models - GPU Optimized**\n",
    "\n",
    "## **Multi-Modal Deep Learning for Meme Stock Prediction**\n",
    "\n",
    "This notebook implements state-of-the-art deep learning models optimized for Google Colab GPU training:\n",
    "\n",
    "- **üìä Transformer-based Sequential Models** - Attention mechanisms for temporal patterns\n",
    "- **üß† Advanced LSTM with Attention** - Bidirectional processing with memory\n",
    "- **üéØ Ensemble System** - Meta-learning combination of models\n",
    "- **üìà XGBoost/LightGBM Integration** - Hybrid traditional-neural approach\n",
    "\n",
    "**Estimated Training Time**: 30-60 minutes with GPU T4  \n",
    "**Requirements**: Upload your `training_data_2021.csv` file\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## **üîß Environment Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies for Colab\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install transformers accelerate\n",
    "!pip install lightgbm xgboost\n",
    "!pip install optuna\n",
    "!pip install scikit-learn pandas numpy matplotlib seaborn plotly\n",
    "!pip install tqdm ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report,\n",
    "    mean_squared_error, mean_absolute_error, r2_score\n",
    ")\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# GPU setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üî• Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è GPU not available, using CPU (training will be slow)\")\n",
    "\n",
    "print(f\"üìä PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## **üìÅ Data Upload and Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload your training data\n",
    "from google.colab import files\n",
    "import io\n",
    "\n",
    "print(\"üì§ Please upload your training_data_2021.csv file:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Load the dataset\n",
    "data = None\n",
    "for filename in uploaded.keys():\n",
    "    print(f\"Loading {filename}...\")\n",
    "    data = pd.read_csv(io.BytesIO(uploaded[filename]))\n",
    "    break\n",
    "\n",
    "if data is None:\n",
    "    raise ValueError(\"No data file uploaded!\")\n",
    "\n",
    "print(f\"\\nüìä **Dataset Overview:**\")\n",
    "print(f\"   Shape: {data.shape}\")\n",
    "print(f\"   Date range: {data['date'].min()} to {data['date'].max()}\")\n",
    "print(f\"   Memory usage: {data.memory_usage(deep=True).sum() / 1e6:.1f} MB\")\n",
    "\n",
    "# Display first few rows\n",
    "print(f\"\\nüìã **Sample Data:**\")\n",
    "display(data.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## **üßπ Advanced Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_preprocessing(df):\n",
    "    \"\"\"Advanced preprocessing optimized for deep learning models\"\"\"\n",
    "    \n",
    "    print(\"üßπ Starting advanced preprocessing...\")\n",
    "    \n",
    "    # Convert date column\n",
    "    if 'date' in df.columns:\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        df = df.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    # Separate features and targets\n",
    "    target_cols = [col for col in df.columns if 'target' in col.lower()]\n",
    "    feature_cols = [col for col in df.columns \n",
    "                   if col not in target_cols + ['date'] \n",
    "                   and df[col].dtype in ['int64', 'float64']]\n",
    "    \n",
    "    print(f\"   üìà Features: {len(feature_cols)}\")\n",
    "    print(f\"   üéØ Targets: {len(target_cols)}\")\n",
    "    \n",
    "    # Extract features and targets\n",
    "    X = df[feature_cols].copy()\n",
    "    y_dict = {}\n",
    "    \n",
    "    # Process target variables\n",
    "    for target in target_cols:\n",
    "        if df[target].dtype in ['int64', 'float64']:\n",
    "            y_dict[target] = df[target].values\n",
    "    \n",
    "    # Handle missing values in features\n",
    "    print(f\"   üîß Missing values: {X.isnull().sum().sum()}\")\n",
    "    if X.isnull().sum().sum() > 0:\n",
    "        # Fill with median for robust handling\n",
    "        X = X.fillna(X.median())\n",
    "        print(f\"   ‚úÖ Filled missing values with median\")\n",
    "    \n",
    "    # Remove constant features\n",
    "    constant_features = X.columns[X.nunique() <= 1]\n",
    "    if len(constant_features) > 0:\n",
    "        X = X.drop(columns=constant_features)\n",
    "        print(f\"   üóëÔ∏è Removed {len(constant_features)} constant features\")\n",
    "    \n",
    "    # Remove highly correlated features (>0.98)\n",
    "    corr_matrix = X.corr().abs()\n",
    "    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    high_corr_features = [col for col in upper_tri.columns if any(upper_tri[col] > 0.98)]\n",
    "    \n",
    "    if high_corr_features:\n",
    "        X = X.drop(columns=high_corr_features)\n",
    "        print(f\"   üîó Removed {len(high_corr_features)} highly correlated features\")\n",
    "    \n",
    "    # Feature scaling using RobustScaler (better for outliers)\n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "    \n",
    "    print(f\"   üìä Final feature shape: {X_scaled.shape}\")\n",
    "    print(f\"   ‚úÖ Applied RobustScaler normalization\")\n",
    "    \n",
    "    return X_scaled, y_dict, scaler, feature_cols, target_cols\n",
    "\n",
    "# Process the data\n",
    "X, y_dict, scaler, feature_cols, target_cols = advanced_preprocessing(data)\n",
    "\n",
    "print(f\"\\nüéØ **Available Targets:**\")\n",
    "for target in target_cols[:5]:  # Show first 5 targets\n",
    "    target_data = y_dict[target]\n",
    "    valid_data = target_data[~np.isnan(target_data)]\n",
    "    if len(valid_data) > 0:\n",
    "        print(f\"   üìä {target}: {len(valid_data)} samples, mean={valid_data.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## **üèóÔ∏è Advanced Model Architectures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    \"\"\"Multi-head attention block with residual connections\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model * 4, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention with residual\n",
    "        attn_out, _ = self.attention(x, x, x, attn_mask=mask)\n",
    "        x = self.norm1(x + attn_out)\n",
    "        \n",
    "        # Feed-forward with residual\n",
    "        ff_out = self.feed_forward(x)\n",
    "        x = self.norm2(x + ff_out)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class AdvancedTransformerModel(nn.Module):\n",
    "    \"\"\"Advanced transformer for sequential meme stock prediction\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, d_model=256, num_heads=8, num_layers=4, \n",
    "                 num_classes=2, sequence_length=30, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_projection = nn.Linear(input_dim, d_model)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(sequence_length, d_model))\n",
    "        \n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            MultiHeadAttentionBlock(d_model, num_heads, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, sequence_length, input_dim)\n",
    "        batch_size, seq_len = x.size(0), x.size(1)\n",
    "        \n",
    "        # Project input to model dimension\n",
    "        x = self.input_projection(x)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        if seq_len <= self.sequence_length:\n",
    "            x = x + self.positional_encoding[:seq_len].unsqueeze(0)\n",
    "        else:\n",
    "            # Handle longer sequences\n",
    "            pos_enc = self.positional_encoding.repeat(seq_len // self.sequence_length + 1, 1)\n",
    "            x = x + pos_enc[:seq_len].unsqueeze(0)\n",
    "        \n",
    "        # Apply transformer blocks\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x)\n",
    "        \n",
    "        # Global attention pooling\n",
    "        attention_weights = torch.softmax(x.mean(dim=-1), dim=1)\n",
    "        x = torch.sum(x * attention_weights.unsqueeze(-1), dim=1)\n",
    "        \n",
    "        # Classification\n",
    "        return self.classifier(x)\n",
    "\n",
    "class AdvancedLSTMModel(nn.Module):\n",
    "    \"\"\"Advanced LSTM with attention and residual connections\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim=128, num_layers=3, \n",
    "                 num_classes=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        lstm_output_dim = hidden_dim * 2  # bidirectional\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=lstm_output_dim,\n",
    "            num_heads=8,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(lstm_output_dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(lstm_output_dim, lstm_output_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(lstm_output_dim // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # LSTM processing\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # Self-attention\n",
    "        attn_out, attn_weights = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "        \n",
    "        # Attention pooling using weights\n",
    "        pooled = torch.mean(attn_out, dim=1)\n",
    "        \n",
    "        return self.classifier(pooled)\n",
    "\n",
    "class HybridNeuralModel(nn.Module):\n",
    "    \"\"\"Hybrid model combining CNN and LSTM for feature extraction\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, num_classes=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1D CNN for local pattern detection\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "        \n",
    "        # LSTM for sequential processing\n",
    "        self.lstm = nn.LSTM(input_dim, 128, num_layers=2, \n",
    "                           bidirectional=True, batch_first=True)\n",
    "        \n",
    "        # Fusion layer\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(256 + 256, 512),  # CNN + LSTM features\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # CNN branch - transpose for Conv1d\n",
    "        cnn_out = self.cnn(x.transpose(1, 2))  # (batch, features, seq) -> (batch, channels, 1)\n",
    "        cnn_features = cnn_out.squeeze(-1)  # (batch, 256)\n",
    "        \n",
    "        # LSTM branch\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_features = lstm_out.mean(dim=1)  # Global average pooling\n",
    "        \n",
    "        # Fusion\n",
    "        combined = torch.cat([cnn_features, lstm_features], dim=1)\n",
    "        return self.fusion(combined)\n",
    "\n",
    "print(\"üèóÔ∏è Advanced model architectures defined!\")\n",
    "print(\"   ‚úÖ MultiHeadAttentionBlock - Self-attention with residuals\")\n",
    "print(\"   ‚úÖ AdvancedTransformerModel - Full transformer stack\")\n",
    "print(\"   ‚úÖ AdvancedLSTMModel - Bidirectional LSTM with attention\")\n",
    "print(\"   ‚úÖ HybridNeuralModel - CNN-LSTM fusion architecture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## **üìä Sequence Data Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(X, y, sequence_length=14, stride=1):\n",
    "    \"\"\"Create sequences for time series modeling\"\"\"\n",
    "    \n",
    "    X_seq = []\n",
    "    y_seq = []\n",
    "    \n",
    "    for i in range(0, len(X) - sequence_length + 1, stride):\n",
    "        # Input sequence\n",
    "        X_seq.append(X[i:i + sequence_length])\n",
    "        # Target (next value after sequence)\n",
    "        if i + sequence_length < len(y):\n",
    "            y_seq.append(y[i + sequence_length])\n",
    "        else:\n",
    "            y_seq.append(y[-1])  # Use last available value\n",
    "    \n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "def prepare_sequence_data(X, y_dict, target_name, sequence_length=14, test_size=0.2):\n",
    "    \"\"\"Prepare sequence data for a specific target\"\"\"\n",
    "    \n",
    "    if target_name not in y_dict:\n",
    "        raise ValueError(f\"Target {target_name} not found in targets\")\n",
    "    \n",
    "    y = y_dict[target_name]\n",
    "    \n",
    "    # Remove NaN values\n",
    "    valid_idx = ~np.isnan(y)\n",
    "    X_valid = X.values[valid_idx]\n",
    "    y_valid = y[valid_idx]\n",
    "    \n",
    "    print(f\"üìä Target: {target_name}\")\n",
    "    print(f\"   Valid samples: {len(y_valid)}\")\n",
    "    print(f\"   Target distribution: mean={y_valid.mean():.3f}, std={y_valid.std():.3f}\")\n",
    "    \n",
    "    if len(y_valid) < sequence_length + 10:\n",
    "        raise ValueError(f\"Insufficient data: {len(y_valid)} samples\")\n",
    "    \n",
    "    # Create sequences\n",
    "    X_seq, y_seq = create_sequences(X_valid, y_valid, sequence_length)\n",
    "    \n",
    "    # Time series split (no shuffling to maintain temporal order)\n",
    "    split_idx = int(len(X_seq) * (1 - test_size))\n",
    "    \n",
    "    X_train, X_test = X_seq[:split_idx], X_seq[split_idx:]\n",
    "    y_train, y_test = y_seq[:split_idx], y_seq[split_idx:]\n",
    "    \n",
    "    print(f\"   Sequences created: {len(X_seq)} total\")\n",
    "    print(f\"   Train: {len(X_train)}, Test: {len(X_test)}\")\n",
    "    \n",
    "    # Convert to tensors\n",
    "    X_train = torch.FloatTensor(X_train)\n",
    "    X_test = torch.FloatTensor(X_test)\n",
    "    \n",
    "    # Handle target type (classification vs regression)\n",
    "    if 'direction' in target_name.lower():\n",
    "        # Classification targets\n",
    "        y_train = torch.LongTensor(y_train.astype(int))\n",
    "        y_test = torch.LongTensor(y_test.astype(int))\n",
    "        task_type = 'classification'\n",
    "        num_classes = len(np.unique(y_valid))\n",
    "    else:\n",
    "        # Regression targets\n",
    "        y_train = torch.FloatTensor(y_train)\n",
    "        y_test = torch.FloatTensor(y_test)\n",
    "        task_type = 'regression'\n",
    "        num_classes = 1\n",
    "    \n",
    "    print(f\"   Task type: {task_type} ({'classes: ' + str(num_classes) if task_type == 'classification' else 'regression'})\")\n",
    "    \n",
    "    return {\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "        'task_type': task_type,\n",
    "        'num_classes': num_classes,\n",
    "        'sequence_length': sequence_length,\n",
    "        'feature_dim': X_train.shape[2]\n",
    "    }\n",
    "\n",
    "# Select target for training (prefer direction targets for classification)\n",
    "direction_targets = [t for t in target_cols if 'direction' in t.lower()]\n",
    "return_targets = [t for t in target_cols if 'return' in t.lower()]\n",
    "\n",
    "if direction_targets:\n",
    "    primary_target = direction_targets[0]\n",
    "elif return_targets:\n",
    "    primary_target = return_targets[0]\n",
    "else:\n",
    "    primary_target = target_cols[0] if target_cols else None\n",
    "\n",
    "if primary_target is None:\n",
    "    raise ValueError(\"No suitable targets found!\")\n",
    "\n",
    "print(f\"üéØ **Selected primary target: {primary_target}**\")\n",
    "\n",
    "# Prepare sequence data\n",
    "sequence_data = prepare_sequence_data(X, y_dict, primary_target, sequence_length=14)\n",
    "\n",
    "print(f\"\\n‚úÖ **Sequence data prepared:**\")\n",
    "print(f\"   üìä Feature dimension: {sequence_data['feature_dim']}\")\n",
    "print(f\"   üìè Sequence length: {sequence_data['sequence_length']}\")\n",
    "print(f\"   üéØ Task: {sequence_data['task_type']}\")\n",
    "print(f\"   üìà Train shape: {sequence_data['X_train'].shape}\")\n",
    "print(f\"   üìä Test shape: {sequence_data['X_test'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## **üöÄ Advanced Model Training Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedTrainer:\n",
    "    \"\"\"Advanced training pipeline with early stopping, learning rate scheduling\"\"\"\n",
    "    \n",
    "    def __init__(self, model, device, task_type='classification'):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.task_type = task_type\n",
    "        self.training_history = {'train_loss': [], 'val_loss': [], 'val_metric': []}\n",
    "        \n",
    "    def train_model(self, X_train, y_train, X_val, y_val, \n",
    "                   epochs=50, batch_size=32, lr=1e-3, patience=10, \n",
    "                   weight_decay=1e-4):\n",
    "        \"\"\"Train model with advanced optimization\"\"\"\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_dataset = TensorDataset(X_train, y_train)\n",
    "        val_dataset = TensorDataset(X_val, y_val)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        # Setup optimizer and scheduler\n",
    "        optimizer = optim.AdamW(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "        )\n",
    "        \n",
    "        # Loss function\n",
    "        if self.task_type == 'classification':\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "        else:\n",
    "            criterion = nn.MSELoss()\n",
    "        \n",
    "        # Early stopping\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        best_model_state = None\n",
    "        \n",
    "        print(f\"üöÄ Starting training for {epochs} epochs...\")\n",
    "        \n",
    "        # Training loop with progress bar\n",
    "        epoch_pbar = tqdm(range(epochs), desc=\"Training Progress\")\n",
    "        \n",
    "        for epoch in epoch_pbar:\n",
    "            # Training phase\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "            \n",
    "            for batch_X, batch_y in train_loader:\n",
    "                batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_X)\n",
    "                \n",
    "                if self.task_type == 'regression':\n",
    "                    outputs = outputs.squeeze()\n",
    "                \n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            train_loss /= len(train_loader)\n",
    "            \n",
    "            # Validation phase\n",
    "            self.model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_metric = 0.0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                all_val_preds = []\n",
    "                all_val_targets = []\n",
    "                \n",
    "                for batch_X, batch_y in val_loader:\n",
    "                    batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)\n",
    "                    outputs = self.model(batch_X)\n",
    "                    \n",
    "                    if self.task_type == 'regression':\n",
    "                        outputs = outputs.squeeze()\n",
    "                    \n",
    "                    loss = criterion(outputs, batch_y)\n",
    "                    val_loss += loss.item()\n",
    "                    \n",
    "                    all_val_preds.append(outputs.cpu())\n",
    "                    all_val_targets.append(batch_y.cpu())\n",
    "                \n",
    "                val_loss /= len(val_loader)\n",
    "                \n",
    "                # Calculate validation metric\n",
    "                all_val_preds = torch.cat(all_val_preds)\n",
    "                all_val_targets = torch.cat(all_val_targets)\n",
    "                \n",
    "                if self.task_type == 'classification':\n",
    "                    pred_classes = torch.argmax(all_val_preds, dim=1)\n",
    "                    val_metric = accuracy_score(all_val_targets.numpy(), pred_classes.numpy())\n",
    "                else:\n",
    "                    val_metric = r2_score(all_val_targets.numpy(), all_val_preds.numpy())\n",
    "            \n",
    "            # Learning rate scheduling\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            # Store history\n",
    "            self.training_history['train_loss'].append(train_loss)\n",
    "            self.training_history['val_loss'].append(val_loss)\n",
    "            self.training_history['val_metric'].append(val_metric)\n",
    "            \n",
    "            # Early stopping check\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                best_model_state = self.model.state_dict().copy()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            metric_name = 'Acc' if self.task_type == 'classification' else 'R¬≤'\n",
    "            epoch_pbar.set_postfix({\n",
    "                'Train Loss': f'{train_loss:.4f}',\n",
    "                'Val Loss': f'{val_loss:.4f}',\n",
    "                f'Val {metric_name}': f'{val_metric:.4f}',\n",
    "                'LR': f'{optimizer.param_groups[0][\"lr\"]:.2e}'\n",
    "            })\n",
    "            \n",
    "            # Early stopping\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\n‚èπÔ∏è Early stopping at epoch {epoch + 1}\")\n",
    "                break\n",
    "        \n",
    "        # Load best model\n",
    "        if best_model_state is not None:\n",
    "            self.model.load_state_dict(best_model_state)\n",
    "        \n",
    "        print(f\"‚úÖ Training completed!\")\n",
    "        print(f\"   Best validation loss: {best_val_loss:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'final_val_metric': val_metric,\n",
    "            'training_history': self.training_history\n",
    "        }\n",
    "    \n",
    "    def evaluate_model(self, X_test, y_test, batch_size=32):\n",
    "        \"\"\"Comprehensive model evaluation\"\"\"\n",
    "        \n",
    "        self.model.eval()\n",
    "        test_dataset = TensorDataset(X_test, y_test)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in test_loader:\n",
    "                batch_X = batch_X.to(self.device)\n",
    "                outputs = self.model(batch_X)\n",
    "                \n",
    "                if self.task_type == 'regression':\n",
    "                    outputs = outputs.squeeze()\n",
    "                \n",
    "                all_preds.append(outputs.cpu())\n",
    "                all_targets.append(batch_y)\n",
    "        \n",
    "        all_preds = torch.cat(all_preds).numpy()\n",
    "        all_targets = torch.cat(all_targets).numpy()\n",
    "        \n",
    "        if self.task_type == 'classification':\n",
    "            pred_classes = np.argmax(all_preds, axis=1)\n",
    "            pred_probs = torch.softmax(torch.tensor(all_preds), dim=1).numpy()\n",
    "            \n",
    "            metrics = {\n",
    "                'accuracy': accuracy_score(all_targets, pred_classes),\n",
    "                'precision': precision_score(all_targets, pred_classes, average='weighted', zero_division=0),\n",
    "                'recall': recall_score(all_targets, pred_classes, average='weighted', zero_division=0),\n",
    "                'f1': f1_score(all_targets, pred_classes, average='weighted', zero_division=0)\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                if len(np.unique(all_targets)) == 2:\n",
    "                    metrics['auc'] = roc_auc_score(all_targets, pred_probs[:, 1])\n",
    "                else:\n",
    "                    metrics['auc'] = roc_auc_score(all_targets, pred_probs, multi_class='ovr', average='weighted')\n",
    "            except:\n",
    "                metrics['auc'] = 0.0\n",
    "        \n",
    "        else:\n",
    "            metrics = {\n",
    "                'mse': mean_squared_error(all_targets, all_preds),\n",
    "                'mae': mean_absolute_error(all_targets, all_preds),\n",
    "                'r2': r2_score(all_targets, all_preds)\n",
    "            }\n",
    "            \n",
    "            # Directional accuracy for returns\n",
    "            if np.var(all_targets) > 0:  # Check if there's variation\n",
    "                direction_acc = accuracy_score(all_targets > 0, all_preds > 0)\n",
    "                metrics['directional_accuracy'] = direction_acc\n",
    "        \n",
    "        return metrics, all_preds, all_targets\n",
    "\n",
    "print(\"üöÄ Advanced training pipeline ready!\")\n",
    "print(\"   ‚úÖ Early stopping with patience\")\n",
    "print(\"   ‚úÖ Learning rate scheduling\")\n",
    "print(\"   ‚úÖ Gradient clipping\")\n",
    "print(\"   ‚úÖ Comprehensive evaluation metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## **üéØ Model Training and Comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    'Transformer': AdvancedTransformerModel(\n",
    "        input_dim=sequence_data['feature_dim'],\n",
    "        d_model=256,\n",
    "        num_heads=8,\n",
    "        num_layers=4,\n",
    "        num_classes=sequence_data['num_classes'],\n",
    "        sequence_length=sequence_data['sequence_length']\n",
    "    ),\n",
    "    'LSTM': AdvancedLSTMModel(\n",
    "        input_dim=sequence_data['feature_dim'],\n",
    "        hidden_dim=128,\n",
    "        num_layers=3,\n",
    "        num_classes=sequence_data['num_classes']\n",
    "    ),\n",
    "    'Hybrid': HybridNeuralModel(\n",
    "        input_dim=sequence_data['feature_dim'],\n",
    "        num_classes=sequence_data['num_classes']\n",
    "    )\n",
    "}\n",
    "\n",
    "print(f\"üèóÔ∏è **Initialized {len(models)} models:**\")\n",
    "for name, model in models.items():\n",
    "    param_count = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"   {name}: {param_count:,} parameters\")\n",
    "\n",
    "# Train each model\n",
    "results = {}\n",
    "trained_models = {}\n",
    "\n",
    "print(f\"\\nüöÄ **Training Models on {sequence_data['task_type']} task...**\")\n",
    "print(f\"Target: {primary_target}\")\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üî• Training {model_name} Model\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = AdvancedTrainer(model, device, sequence_data['task_type'])\n",
    "    \n",
    "    # Train model\n",
    "    train_results = trainer.train_model(\n",
    "        X_train=sequence_data['X_train'],\n",
    "        y_train=sequence_data['y_train'],\n",
    "        X_val=sequence_data['X_test'][:len(sequence_data['X_test'])//2],  # Use part of test as validation\n",
    "        y_val=sequence_data['y_test'][:len(sequence_data['y_test'])//2],\n",
    "        epochs=30,\n",
    "        batch_size=16,\n",
    "        lr=1e-3,\n",
    "        patience=8\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_metrics, predictions, targets = trainer.evaluate_model(\n",
    "        X_test=sequence_data['X_test'][len(sequence_data['X_test'])//2:],  # Use remaining test data\n",
    "        y_test=sequence_data['y_test'][len(sequence_data['y_test'])//2:]\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    results[model_name] = {\n",
    "        'train_results': train_results,\n",
    "        'test_metrics': test_metrics,\n",
    "        'predictions': predictions,\n",
    "        'targets': targets\n",
    "    }\n",
    "    trained_models[model_name] = trainer.model\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nüìä {model_name} Results:\")\n",
    "    for metric, value in test_metrics.items():\n",
    "        print(f\"   {metric.title()}: {value:.4f}\")\n",
    "\n",
    "print(f\"\\nüéâ All models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## **ü§ù Ensemble Model Creation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedEnsemble(nn.Module):\n",
    "    \"\"\"Advanced ensemble with learned weights\"\"\"\n",
    "    \n",
    "    def __init__(self, models, num_classes, device):\n",
    "        super().__init__()\n",
    "        self.models = nn.ModuleList(models)\n",
    "        self.device = device\n",
    "        \n",
    "        # Learned ensemble weights\n",
    "        self.ensemble_weights = nn.Parameter(torch.ones(len(models)) / len(models))\n",
    "        \n",
    "        # Meta-learner (optional)\n",
    "        self.meta_learner = nn.Sequential(\n",
    "            nn.Linear(len(models) * num_classes, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, use_meta_learner=False):\n",
    "        # Get predictions from all models\n",
    "        model_outputs = []\n",
    "        \n",
    "        for model in self.models:\n",
    "            with torch.no_grad():\n",
    "                output = model(x)\n",
    "                if len(output.shape) == 1:\n",
    "                    output = output.unsqueeze(1)\n",
    "                model_outputs.append(output)\n",
    "        \n",
    "        if use_meta_learner:\n",
    "            # Meta-learning approach\n",
    "            concatenated = torch.cat(model_outputs, dim=1)\n",
    "            return self.meta_learner(concatenated)\n",
    "        else:\n",
    "            # Weighted averaging\n",
    "            weights = torch.softmax(self.ensemble_weights, dim=0)\n",
    "            weighted_outputs = []\n",
    "            \n",
    "            for i, output in enumerate(model_outputs):\n",
    "                weighted_outputs.append(weights[i] * output)\n",
    "            \n",
    "            return torch.stack(weighted_outputs).sum(dim=0)\n",
    "\n",
    "# Create ensemble\n",
    "ensemble_models = list(trained_models.values())\n",
    "ensemble = AdvancedEnsemble(\n",
    "    models=ensemble_models,\n",
    "    num_classes=sequence_data['num_classes'],\n",
    "    device=device\n",
    ").to(device)\n",
    "\n",
    "print(f\"ü§ù Created ensemble with {len(ensemble_models)} models\")\n",
    "\n",
    "# Train ensemble weights\n",
    "ensemble_trainer = AdvancedTrainer(ensemble, device, sequence_data['task_type'])\n",
    "\n",
    "print(\"\\nüî• Training ensemble weights...\")\n",
    "ensemble_results = ensemble_trainer.train_model(\n",
    "    X_train=sequence_data['X_train'],\n",
    "    y_train=sequence_data['y_train'],\n",
    "    X_val=sequence_data['X_test'][:len(sequence_data['X_test'])//2],\n",
    "    y_val=sequence_data['y_test'][:len(sequence_data['y_test'])//2],\n",
    "    epochs=20,\n",
    "    batch_size=16,\n",
    "    lr=1e-4,\n",
    "    patience=5\n",
    ")\n",
    "\n",
    "# Evaluate ensemble\n",
    "ensemble_metrics, ensemble_preds, ensemble_targets = ensemble_trainer.evaluate_model(\n",
    "    X_test=sequence_data['X_test'][len(sequence_data['X_test'])//2:],\n",
    "    y_test=sequence_data['y_test'][len(sequence_data['y_test'])//2:]\n",
    ")\n",
    "\n",
    "# Store ensemble results\n",
    "results['Ensemble'] = {\n",
    "    'train_results': ensemble_results,\n",
    "    'test_metrics': ensemble_metrics,\n",
    "    'predictions': ensemble_preds,\n",
    "    'targets': ensemble_targets\n",
    "}\n",
    "\n",
    "print(f\"\\nüìä Ensemble Results:\")\n",
    "for metric, value in ensemble_metrics.items():\n",
    "    print(f\"   {metric.title()}: {value:.4f}\")\n",
    "\n",
    "# Print ensemble weights\n",
    "weights = torch.softmax(ensemble.ensemble_weights, dim=0)\n",
    "print(f\"\\nüéØ Learned Ensemble Weights:\")\n",
    "for i, (name, weight) in enumerate(zip(trained_models.keys(), weights)):\n",
    "    print(f\"   {name}: {weight.item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## **üìä Results Visualization and Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive results summary\n",
    "summary_data = []\n",
    "\n",
    "for model_name, result in results.items():\n",
    "    metrics = result['test_metrics']\n",
    "    \n",
    "    # Get primary metric based on task type\n",
    "    if sequence_data['task_type'] == 'classification':\n",
    "        primary_metric = metrics.get('accuracy', 0)\n",
    "        metric_name = 'Accuracy'\n",
    "    else:\n",
    "        primary_metric = metrics.get('r2', 0)\n",
    "        metric_name = 'R¬≤'\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Model': model_name,\n",
    "        metric_name: primary_metric,\n",
    "        'Type': 'Ensemble' if model_name == 'Ensemble' else 'Individual'\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(summary_data)\n",
    "print(\"üìä **Final Model Comparison:**\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Create visualizations\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=(\n",
    "        'Model Performance Comparison',\n",
    "        'Training History (Best Model)',\n",
    "        'Prediction vs Actual (Best Model)',\n",
    "        'Feature Importance Analysis'\n",
    "    ),\n",
    "    specs=[[{\"type\": \"bar\"}, {\"type\": \"scatter\"}],\n",
    "           [{\"type\": \"scatter\"}, {\"type\": \"bar\"}]]\n",
    ")\n",
    "\n",
    "# 1. Model Performance Comparison\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=results_df['Model'],\n",
    "        y=results_df[metric_name],\n",
    "        text=[f'{v:.3f}' for v in results_df[metric_name]],\n",
    "        textposition='auto',\n",
    "        marker_color=['#FF6B6B' if t == 'Ensemble' else '#4ECDC4' for t in results_df['Type']]\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# 2. Training History (best performing model)\n",
    "best_model_name = results_df.loc[results_df[metric_name].idxmax(), 'Model']\n",
    "best_history = results[best_model_name]['train_results']['training_history']\n",
    "\n",
    "epochs = range(1, len(best_history['train_loss']) + 1)\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(epochs),\n",
    "        y=best_history['train_loss'],\n",
    "        mode='lines',\n",
    "        name='Train Loss',\n",
    "        line=dict(color='#FF6B6B')\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(epochs),\n",
    "        y=best_history['val_loss'],\n",
    "        mode='lines',\n",
    "        name='Val Loss',\n",
    "        line=dict(color='#4ECDC4')\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# 3. Prediction vs Actual (best model)\n",
    "best_preds = results[best_model_name]['predictions']\n",
    "best_targets = results[best_model_name]['targets']\n",
    "\n",
    "if sequence_data['task_type'] == 'classification':\n",
    "    # For classification, show confusion matrix style\n",
    "    pred_classes = np.argmax(best_preds, axis=1) if len(best_preds.shape) > 1 else best_preds\n",
    "    from collections import Counter\n",
    "    \n",
    "    # Count predictions vs actuals\n",
    "    comparison = [(int(t), int(p)) for t, p in zip(best_targets, pred_classes)]\n",
    "    counter = Counter(comparison)\n",
    "    \n",
    "    scatter_data = [(k[0], k[1], v) for k, v in counter.items()]\n",
    "    if scatter_data:\n",
    "        x_vals, y_vals, sizes = zip(*scatter_data)\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=x_vals,\n",
    "                y=y_vals,\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=[s*5 for s in sizes],  # Scale for visibility\n",
    "                    color=sizes,\n",
    "                    colorscale='Viridis',\n",
    "                    showscale=True\n",
    "                ),\n",
    "                text=[f'Count: {s}' for s in sizes],\n",
    "                hovertemplate='Actual: %{x}<br>Predicted: %{y}<br>%{text}<extra></extra>'\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "else:\n",
    "    # For regression, scatter plot\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=best_targets,\n",
    "            y=best_preds,\n",
    "            mode='markers',\n",
    "            marker=dict(color='#45B7D1', size=6, opacity=0.6),\n",
    "            name='Predictions'\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    min_val, max_val = min(best_targets.min(), best_preds.min()), max(best_targets.max(), best_preds.max())\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[min_val, max_val],\n",
    "            y=[min_val, max_val],\n",
    "            mode='lines',\n",
    "            line=dict(dash='dash', color='red'),\n",
    "            name='Perfect Prediction'\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "# 4. Feature Importance (using a simple gradient-based approach)\n",
    "try:\n",
    "    # Simple feature importance based on input gradients\n",
    "    best_model = trained_models[best_model_name]\n",
    "    best_model.eval()\n",
    "    \n",
    "    # Sample batch for gradient calculation\n",
    "    sample_X = sequence_data['X_test'][:10].to(device)\n",
    "    sample_X.requires_grad_()\n",
    "    \n",
    "    outputs = best_model(sample_X)\n",
    "    if sequence_data['task_type'] == 'classification':\n",
    "        outputs = outputs.max(dim=1)[0]  # Max class score\n",
    "    else:\n",
    "        outputs = outputs.squeeze()\n",
    "    \n",
    "    gradients = torch.autograd.grad(outputs.sum(), sample_X)[0]\n",
    "    importance = gradients.abs().mean(dim=(0, 1)).cpu().numpy()\n",
    "    \n",
    "    # Get top features\n",
    "    top_indices = np.argsort(importance)[-10:]\n",
    "    top_importance = importance[top_indices]\n",
    "    top_features = [f'Feature_{i}' for i in top_indices]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=top_importance,\n",
    "            y=top_features,\n",
    "            orientation='h',\n",
    "            marker_color='#95E1D3'\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Feature importance calculation failed: {e}\")\n",
    "    # Add placeholder\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=[1],\n",
    "            y=['Feature importance unavailable'],\n",
    "            orientation='h',\n",
    "            marker_color='#95E1D3'\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=f'üöÄ Advanced Meme Stock Prediction Models - {sequence_data[\"task_type\"].title()} Task',\n",
    "    showlegend=False,\n",
    "    height=800\n",
    ")\n",
    "\n",
    "# Update axes labels\n",
    "fig.update_xaxes(title_text=\"Model\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=metric_name, row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Epoch\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Loss\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"Actual\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Predicted\", row=2, col=1)\n",
    "fig.update_xaxes(title_text=\"Importance\", row=2, col=2)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(f\"\\nüèÜ **Best Model: {best_model_name}**\")\n",
    "print(f\"   {metric_name}: {results_df.loc[results_df[metric_name].idxmax(), metric_name]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## **üíæ Model Export and Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models and results\n",
    "print(\"üíæ Saving trained models and results...\")\n",
    "\n",
    "# Save model checkpoints\n",
    "for model_name, model in trained_models.items():\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'model_config': {\n",
    "            'input_dim': sequence_data['feature_dim'],\n",
    "            'num_classes': sequence_data['num_classes'],\n",
    "            'task_type': sequence_data['task_type']\n",
    "        },\n",
    "        'metrics': results[model_name]['test_metrics'],\n",
    "        'target': primary_target\n",
    "    }, f'{model_name.lower()}_model.pth')\n",
    "    print(f\"‚úÖ Saved {model_name} model\")\n",
    "\n",
    "# Save ensemble\n",
    "torch.save({\n",
    "    'ensemble_state_dict': ensemble.state_dict(),\n",
    "    'component_models': list(trained_models.keys()),\n",
    "    'ensemble_weights': torch.softmax(ensemble.ensemble_weights, dim=0).detach().cpu().numpy(),\n",
    "    'metrics': results['Ensemble']['test_metrics'],\n",
    "    'target': primary_target\n",
    "}, 'ensemble_model.pth')\n",
    "print(\"‚úÖ Saved ensemble model\")\n",
    "\n",
    "# Save scaler\n",
    "import joblib\n",
    "joblib.dump(scaler, 'feature_scaler.pkl')\n",
    "print(\"‚úÖ Saved feature scaler\")\n",
    "\n",
    "# Save detailed results\n",
    "detailed_results = {\n",
    "    'task_type': sequence_data['task_type'],\n",
    "    'target': primary_target,\n",
    "    'sequence_length': sequence_data['sequence_length'],\n",
    "    'feature_dim': sequence_data['feature_dim'],\n",
    "    'num_classes': sequence_data['num_classes'],\n",
    "    'model_results': {}\n",
    "}\n",
    "\n",
    "for model_name, result in results.items():\n",
    "    detailed_results['model_results'][model_name] = {\n",
    "        'test_metrics': result['test_metrics'],\n",
    "        'final_val_metric': result['train_results']['final_val_metric']\n",
    "    }\n",
    "\n",
    "# Save to JSON\n",
    "with open('training_results.json', 'w') as f:\n",
    "    json.dump(detailed_results, f, indent=2, default=str)\n",
    "print(\"‚úÖ Saved detailed results\")\n",
    "\n",
    "# Save results CSV\n",
    "results_df.to_csv('model_comparison.csv', index=False)\n",
    "print(\"‚úÖ Saved results CSV\")\n",
    "\n",
    "# Create comprehensive summary\n",
    "print(f\"\\nüìã **Training Summary:**\")\n",
    "print(f\"   üéØ Target: {primary_target}\")\n",
    "print(f\"   üìä Task Type: {sequence_data['task_type'].title()}\")\n",
    "print(f\"   üìà Models Trained: {len(trained_models)}\")\n",
    "print(f\"   ü§ù Ensemble Created: Yes\")\n",
    "print(f\"   üèÜ Best Model: {best_model_name}\")\n",
    "print(f\"   üìä Best {metric_name}: {results_df[metric_name].max():.4f}\")\n",
    "\n",
    "# Download files for Colab\n",
    "print(\"\\nüì• **Download Results:**\")\n",
    "files_to_download = [\n",
    "    'model_comparison.csv',\n",
    "    'training_results.json',\n",
    "    'feature_scaler.pkl',\n",
    "    f'{best_model_name.lower()}_model.pth',\n",
    "    'ensemble_model.pth'\n",
    "]\n",
    "\n",
    "for filename in files_to_download:\n",
    "    if os.path.exists(filename):\n",
    "        files.download(filename)\n",
    "        print(f\"‚¨áÔ∏è Downloaded: {filename}\")\n",
    "\n",
    "print(f\"\\nüéâ **Advanced Model Training Complete!**\")\n",
    "print(f\"\\nüí° **Key Achievements:**\")\n",
    "print(f\"   ‚úÖ Trained {len(trained_models)} state-of-the-art deep learning models\")\n",
    "print(f\"   ‚úÖ Created intelligent ensemble with learned weights\")\n",
    "print(f\"   ‚úÖ Achieved {metric_name.lower()} of {results_df[metric_name].max():.4f}\")\n",
    "print(f\"   ‚úÖ Implemented advanced training techniques (early stopping, scheduling)\")\n",
    "print(f\"   ‚úÖ Generated comprehensive visualizations and analysis\")\n",
    "print(f\"\\nüöÄ **Ready for production deployment!**\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}