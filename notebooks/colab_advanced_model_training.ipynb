{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# ðŸš€ **Week 2 Advanced Model Training - Day 10-11**\n",
        "\n",
        "## **Multi-Modal Transformer & Advanced Ensemble Training**\n",
        "\n",
        "This notebook implements the advanced model architectures for Week 2:\n",
        "- **BERT Sentiment Pipeline** (FinBERT)\n",
        "- **Multi-Modal Transformer Architecture**\n",
        "- **Advanced LSTM with Attention**\n",
        "- **Ensemble System Training**\n",
        "\n",
        "**Estimated Training Time**: 4-6 hours with GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## **1. Environment Setup & Dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "# Install required dependencies\n",
        "!pip install transformers torch torchvision torchaudio\n",
        "!pip install sentence-transformers\n",
        "!pip install optuna\n",
        "!pip install lightgbm xgboost\n",
        "!pip install scikit-learn pandas numpy matplotlib seaborn\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "import os  # [FIX]\n",
        "import random  # [FIX]\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import AdamW  # [FIX] use AdamW from torch.optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from transformers import get_linear_schedule_with_warmup  # [FIX]\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    mean_squared_error,\n",
        "    confusion_matrix,  # [FIX]\n",
        "    roc_auc_score,  # [FIX]\n",
        ")\n",
        "import optuna\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# [FIX] Ensure deterministic behavior\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Check GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_upload"
      },
      "source": [
        "## **2. Data Upload & Loading**\n",
        "\n",
        "**Upload your advanced features dataset** (colab_advanced_features.csv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_data"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "# Upload your dataset\n",
        "print(\"Please upload your advanced features dataset (colab_advanced_features.csv)\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Load the dataset\n",
        "for filename in uploaded.keys():\n",
        "    print(f\"Loading {filename}...\")\n",
        "    data = pd.read_csv(io.BytesIO(uploaded[filename]))\n",
        "    \n",
        "print(f\"Dataset shape: {data.shape}\")\n",
        "print(f\"Columns: {len(data.columns)}\")\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_prep"
      },
      "source": [
        "## **3. Data Preparation & Feature Engineering**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prepare_features"
      },
      "outputs": [],
      "source": [
        "def prepare_advanced_features(data):\n",
        "    \"\"\"Prepare features for advanced model training\"\"\"\n",
        "    \n",
        "    # Separate features and targets\n",
        "    feature_cols = [col for col in data.columns if not any(x in col for x in \n",
        "                    ['direction', 'magnitude', 'returns', 'target'])]\n",
        "    \n",
        "    # Target variables\n",
        "    target_cols = [col for col in data.columns if any(x in col for x in \n",
        "                   ['direction', 'magnitude'])]\n",
        "    \n",
        "    print(f\"Feature columns: {len(feature_cols)}\")\n",
        "    print(f\"Target columns: {len(target_cols)}\")\n",
        "    \n",
        "    # Remove any text columns for now (we'll handle BERT separately)\n",
        "    numeric_features = data[feature_cols].select_dtypes(include=[np.number])\n",
        "    \n",
        "    # Handle missing values\n",
        "    numeric_features = numeric_features.fillna(0)\n",
        "    \n",
        "    return numeric_features, target_cols, data\n",
        "\n",
        "# Prepare features\n",
        "features, target_cols, full_data = prepare_advanced_features(data)\n",
        "print(f\"\\nNumeric features shape: {features.shape}\")\n",
        "print(f\"Target columns: {target_cols}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bert_sentiment"
      },
      "source": [
        "## **4. BERT Sentiment Analysis Pipeline**\n",
        "\n",
        "**Task 1**: Train FinBERT for financial sentiment analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bert_setup"
      },
      "outputs": [],
      "source": [
        "class FinancialBERTClassifier(nn.Module):\n",
        "    def __init__(self, model_name='ProsusAI/finbert', num_classes=3, pooling='cls'):\n",
        "        super().__init__()\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
        "        self.pooling = pooling  # [FIX]\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden_state = outputs.last_hidden_state  # [FIX]\n",
        "        if self.pooling == 'mean':  # [FIX]\n",
        "            # Attention-mask aware mean pooling\n",
        "            mask = attention_mask.unsqueeze(-1).float()\n",
        "            summed = torch.sum(last_hidden_state * mask, dim=1)\n",
        "            denom = torch.clamp(mask.sum(dim=1), min=1e-6)\n",
        "            pooled_output = summed / denom\n",
        "        else:  # 'cls'\n",
        "            pooled_output = last_hidden_state[:, 0, :]\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits\n",
        "\n",
        "# Initialize model and tokenizer\n",
        "print(\"Initializing FinBERT model...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained('ProsusAI/finbert')  # [FIX]\n",
        "bert_model = FinancialBERTClassifier(pooling='cls').to(device)  # [FIX]\n",
        "print(\"âœ… FinBERT model initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [FIX] 4-12: Robust FinBERT training with proper tokenization, DataLoader, AMP, scheduler, early stopping, logging\n",
        "import time  # [FIX]\n",
        "from dataclasses import dataclass  # [FIX]\n",
        "\n",
        "# [FIX] Reproducibility\n",
        "SEED = 42\n",
        "\n",
        "def set_seed(seed: int = 42):  # [FIX]\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(SEED)  # [FIX]\n",
        "\n",
        "# [FIX] Detect text and label columns\n",
        "candidate_text_cols = [\n",
        "    c for c in data.columns\n",
        "    if data[c].dtype == object and any(k in c.lower() for k in ['text','content','body','title','message','post','headline'])\n",
        "]\n",
        "text_col = candidate_text_cols[0] if candidate_text_cols else None\n",
        "if text_col is None:  # fallback: join all non-target object columns\n",
        "    non_target_obj = [c for c in data.columns if data[c].dtype == object and not any(x in c for x in ['direction','magnitude','returns','target','label','sentiment'])]\n",
        "    if len(non_target_obj) > 0:\n",
        "        text_col = non_target_obj[0]\n",
        "    else:\n",
        "        # create synthetic text by joining row values\n",
        "        text_col = '__auto_text__'\n",
        "        data[text_col] = data.astype(str).agg(' '.join, axis=1)\n",
        "\n",
        "label_col_candidates = [c for c in data.columns if any(k in c.lower() for k in ['sentiment','label'])]\n",
        "if len(label_col_candidates) == 0:\n",
        "    # fallback to direction columns\n",
        "    dir_cols = [c for c in data.columns if 'direction' in c.lower()]\n",
        "    if len(dir_cols) == 0:\n",
        "        raise ValueError(\"No label column found. Please include a 'sentiment'/'label' or a '*direction*' column in the dataset.\")\n",
        "    label_col = dir_cols[0]\n",
        "else:\n",
        "    label_col = label_col_candidates[0]\n",
        "\n",
        "# [FIX] Map labels to 3 classes if needed\n",
        "labels_raw = data[label_col].values\n",
        "if labels_raw.dtype.kind in {'U', 'S', 'O'}:\n",
        "    # categorical strings\n",
        "    unique_vals = sorted(pd.Series(labels_raw).astype(str).unique())\n",
        "    label2id = {v: i for i, v in enumerate(unique_vals)}\n",
        "    y_all = np.array([label2id[str(v)] for v in labels_raw])\n",
        "else:\n",
        "    # numeric -> map to {-1,0,1} -> {0,1,2}\n",
        "    vals = pd.Series(labels_raw).astype(float).values\n",
        "    mapped = np.sign(vals)\n",
        "    mapped = mapped.astype(int)\n",
        "    y_all = (mapped + 1).astype(int)\n",
        "    unique_vals = sorted(np.unique(y_all))\n",
        "    label2id = {int(v): int(v) for v in unique_vals}\n",
        "\n",
        "y_num_classes = int(max(y_all)) + 1\n",
        "id2label = {v: k for k, v in label2id.items()}\n",
        "\n",
        "print(f\"[FIX] Using text column: {text_col}, label column: {label_col}, num_classes={y_num_classes}\")\n",
        "\n",
        "# [FIX] Ensure classifier output matches dataset classes\n",
        "if getattr(bert_model.classifier, 'out_features', None) != y_num_classes:\n",
        "    in_features = bert_model.classifier.in_features\n",
        "    bert_model.classifier = nn.Linear(in_features, y_num_classes).to(device)  # [FIX]\n",
        "\n",
        "# [FIX] Dataset without tensor conversion\n",
        "class BERTSentimentDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'text': str(self.texts[idx]),\n",
        "            'label': int(self.labels[idx]),\n",
        "        }\n",
        "\n",
        "# [FIX] Collate using tokenizer.pad for dynamic padding\n",
        "@dataclass\n",
        "class CollateConfig:\n",
        "    tokenizer: any\n",
        "    max_length: int = 128\n",
        "\n",
        "collate_cfg = CollateConfig(tokenizer=tokenizer, max_length=128)\n",
        "\n",
        "def collate_fn(batch):  # [FIX]\n",
        "    texts = [b['text'] for b in batch]\n",
        "    labels = torch.tensor([b['label'] for b in batch], dtype=torch.long)\n",
        "    encoded_list = [\n",
        "        collate_cfg.tokenizer(\n",
        "            t,\n",
        "            truncation=True,\n",
        "            max_length=collate_cfg.max_length,\n",
        "            add_special_tokens=True,\n",
        "            return_attention_mask=True,\n",
        "            return_token_type_ids=False,\n",
        "        )\n",
        "        for t in texts\n",
        "    ]\n",
        "    enc = collate_cfg.tokenizer.pad(\n",
        "        encoded_list,\n",
        "        padding=True,\n",
        "        max_length=collate_cfg.max_length,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    return enc, labels\n",
        "\n",
        "# [FIX] TimeSeriesSplit without shuffle; ensure no future leakage\n",
        "tscv = TimeSeriesSplit(n_splits=3)\n",
        "indices = np.arange(len(data))\n",
        "splits = list(tscv.split(indices))\n",
        "train_idx, valid_idx = splits[-1]  # use last split for validation\n",
        "assert train_idx.max() < valid_idx.min(), \"[FIX] Time order violated between train and valid (potential leakage)\"  # [FIX]\n",
        "\n",
        "train_texts = data.iloc[train_idx][text_col].astype(str).tolist()\n",
        "train_labels = y_all[train_idx]\n",
        "valid_texts = data.iloc[valid_idx][text_col].astype(str).tolist()\n",
        "valid_labels = y_all[valid_idx]\n",
        "\n",
        "train_dataset = BERTSentimentDataset(train_texts, train_labels)\n",
        "valid_dataset = BERTSentimentDataset(valid_texts, valid_labels)\n",
        "\n",
        "BATCH_SIZE = 16  # [FIX]\n",
        "\n",
        "def seed_worker(worker_id):  # [FIX]\n",
        "    worker_seed = SEED + worker_id\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "generator = torch.Generator()\n",
        "generator.manual_seed(SEED)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,  # [FIX] to prevent leakage-like reordering\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_fn,\n",
        "    worker_init_fn=seed_worker,\n",
        "    generator=generator,\n",
        "    drop_last=False,\n",
        ")\n",
        "valid_loader = DataLoader(\n",
        "    valid_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,  # [FIX]\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_fn,\n",
        "    worker_init_fn=seed_worker,\n",
        "    generator=generator,\n",
        "    drop_last=False,\n",
        ")\n",
        "\n",
        "# [FIX] Guard/logging\n",
        "EPOCHS = 5\n",
        "print(f\"[FIX] epochs={EPOCHS}, len(train_dataset)={len(train_dataset)}, len(train_loader)={len(train_loader)}, len(valid_dataset)={len(valid_dataset)}, len(valid_loader)={len(valid_loader)}\")\n",
        "first_batch_inputs, first_batch_labels = next(iter(train_loader))\n",
        "for k, v in first_batch_inputs.items():\n",
        "    print(f\"[FIX] first batch {k} shape: {tuple(v.shape)}\")\n",
        "print(f\"[FIX] first batch labels shape: {tuple(first_batch_labels.shape)}\")\n",
        "\n",
        "# [FIX] Optimizer param groups with weight decay\n",
        "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "param_groups = [\n",
        "    {\"params\": [p for n, p in bert_model.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": 0.01},\n",
        "    {\"params\": [p for n, p in bert_model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
        "]\n",
        "optimizer = AdamW(param_groups, lr=2e-5)  # [FIX]\n",
        "\n",
        "# [FIX] Scheduler with warmup\n",
        "TOTAL_STEPS = max(1, len(train_loader) * EPOCHS)\n",
        "WARMUP_STEPS = int(0.1 * TOTAL_STEPS)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, WARMUP_STEPS, TOTAL_STEPS)\n",
        "\n",
        "# [FIX] AMP + GradScaler\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# [FIX] EarlyStopping\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=3, mode='max'):\n",
        "        self.patience = patience\n",
        "        self.mode = mode\n",
        "        self.best = None\n",
        "        self.counter = 0\n",
        "        self.best_state = None\n",
        "        self.best_epoch = -1\n",
        "    def step(self, value, model, epoch):\n",
        "        improved = False\n",
        "        if self.best is None:\n",
        "            self.best = value\n",
        "            improved = True\n",
        "        else:\n",
        "            if (self.mode == 'max' and value > self.best) or (self.mode == 'min' and value < self.best):\n",
        "                improved = True\n",
        "        if improved:\n",
        "            self.best = value\n",
        "            self.counter = 0\n",
        "            self.best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "            self.best_epoch = epoch\n",
        "        else:\n",
        "            self.counter += 1\n",
        "        return improved\n",
        "\n",
        "# [FIX] Training loop with tqdm, AMP, grad clipping, logging\n",
        "opt_steps = 0\n",
        "start_time = time.time()\n",
        "progress_epochs = tqdm(range(EPOCHS), desc='[FIX] Epochs')\n",
        "best_val_acc = None\n",
        "best_epoch = -1\n",
        "scaler_step_count = 0\n",
        "scaler_update_count = 0\n",
        "\n",
        "for epoch in progress_epochs:\n",
        "    bert_model.train()  # [FIX]\n",
        "    batch_iter = tqdm(train_loader, desc=f\"[FIX] Train epoch {epoch}\", leave=False)\n",
        "    for batch in batch_iter:\n",
        "        inputs, labels = batch\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        labels = labels.to(device)\n",
        "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):  # [FIX]\n",
        "            logits = bert_model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
        "            loss = criterion(logits, labels)\n",
        "        scaler.scale(loss).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(bert_model.parameters(), 1.0)  # [FIX]\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scaler_step_count += 1\n",
        "        scaler_update_count += 1\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        scheduler.step()\n",
        "        opt_steps += 1\n",
        "        batch_iter.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
        "\n",
        "    # Validation\n",
        "    bert_model.eval()\n",
        "    val_preds = []\n",
        "    val_probs = []\n",
        "    val_labels = []\n",
        "    with torch.no_grad():  # [FIX] only in eval\n",
        "        for inputs, labels in tqdm(valid_loader, desc=f\"[FIX] Valid epoch {epoch}\", leave=False):\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "            labels = labels.to(device)\n",
        "            logits = bert_model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
        "            probs = torch.softmax(logits, dim=1)\n",
        "            preds = torch.argmax(probs, dim=1)\n",
        "            val_probs.append(probs.detach().cpu().numpy())\n",
        "            val_preds.append(preds.detach().cpu().numpy())\n",
        "            val_labels.append(labels.detach().cpu().numpy())\n",
        "    val_probs = np.concatenate(val_probs, axis=0)\n",
        "    val_preds = np.concatenate(val_preds, axis=0)\n",
        "    val_labels_np = np.concatenate(val_labels, axis=0)\n",
        "\n",
        "    val_acc = accuracy_score(val_labels_np, val_preds)\n",
        "\n",
        "    try:\n",
        "        if y_num_classes == 2:\n",
        "            auc = roc_auc_score(val_labels_np, val_probs[:, 1])\n",
        "        else:\n",
        "            auc = roc_auc_score(val_labels_np, val_probs, multi_class='ovr', average='macro')\n",
        "    except Exception:\n",
        "        auc = None\n",
        "\n",
        "    progress_epochs.set_postfix({\"val_acc\": f\"{val_acc:.4f}\", \"auc\": f\"{auc if auc is not None else 'NA'}\"})\n",
        "\n",
        "    if best_val_acc is None or val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_epoch = epoch\n",
        "\n",
        "    # Early stopping\n",
        "    # Using separate early stopper to keep best_state\n",
        "    if epoch == 0:\n",
        "        early_stopper = EarlyStopping(patience=3, mode='max')\n",
        "    early_stopper.step(val_acc, bert_model, epoch)\n",
        "    if early_stopper.counter >= early_stopper.patience:\n",
        "        print(f\"[FIX] Early stopping triggered at epoch {epoch}\")\n",
        "        break\n",
        "\n",
        "train_time_sec = time.time() - start_time\n",
        "\n",
        "print(f\"[FIX] optimizer steps = {opt_steps}\")\n",
        "print(f\"[FIX] optimizer steps >= len(train_loader) ? {opt_steps >= len(train_loader)} (len(train_loader)={len(train_loader)})\")  # [FIX]\n",
        "print(f\"[FIX] scaler steps = {scaler_step_count}, scaler updates = {scaler_update_count}\")\n",
        "\n",
        "# [FIX] Assertions\n",
        "assert EPOCHS > 0 and len(train_loader) > 0 and opt_steps > 0 and opt_steps >= len(train_loader)\n",
        "\n",
        "# [FIX] Evaluate pooling option 'mean' vs 'cls' on validation (no retraining)\n",
        "original_pooling = bert_model.pooling if hasattr(bert_model, 'pooling') else 'cls'\n",
        "bert_model.pooling = 'mean'\n",
        "bert_model.eval()\n",
        "val_preds_mean = []\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in valid_loader:\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        logits = bert_model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
        "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "        val_preds_mean.append(preds)\n",
        "val_preds_mean = np.concatenate(val_preds_mean)\n",
        "val_acc_mean = accuracy_score(val_labels_np, val_preds_mean)\n",
        "bert_model.pooling = original_pooling\n",
        "\n",
        "best_pooling = 'mean' if val_acc_mean > best_val_acc else original_pooling\n",
        "best_pool_val = max(best_val_acc, val_acc_mean)\n",
        "assert 0.0 <= best_pool_val <= 1.0  # [FIX]\n",
        "print(f\"[FIX] Pooling comparison -> cls_acc={best_val_acc:.4f}, mean_acc={val_acc_mean:.4f}; best='{best_pooling}'\")\n",
        "\n",
        "# [FIX] Reports\n",
        "print(\"[FIX] Classification report (validation):\")\n",
        "print(classification_report(val_labels_np, val_preds, zero_division=0))\n",
        "try:\n",
        "    cm = confusion_matrix(val_labels_np, val_preds)\n",
        "    print(\"[FIX] Confusion matrix:\\n\", cm)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# [FIX] Save best checkpoint\n",
        "best_state = early_stopper.best_state if early_stopper.best_state is not None else bert_model.state_dict()\n",
        "bert_model.load_state_dict(best_state)\n",
        "\n",
        "hyperparams = {\n",
        "    \"lr\": 2e-5,\n",
        "    \"weight_decay\": 0.01,\n",
        "    \"batch_size\": BATCH_SIZE,\n",
        "    \"max_len\": collate_cfg.max_length,\n",
        "    \"epochs\": EPOCHS,\n",
        "    \"warmup_ratio\": 0.1,\n",
        "    \"pooling\": best_pooling,  # [FIX] save best pooling\n",
        "    \"seed\": SEED,\n",
        "}\n",
        "\n",
        "ckpt = {\n",
        "    \"state_dict\": best_state,\n",
        "    \"model_name\": \"ProsusAI/finbert\",\n",
        "    \"label2id\": label2id,\n",
        "    \"id2label\": id2label,\n",
        "    \"hyperparams\": hyperparams,\n",
        "    \"best_val\": float(best_pool_val),\n",
        "    \"best_epoch\": int(best_epoch),\n",
        "}\n",
        "\n",
        "torch.save(ckpt, \"finbert_week2.pt\")  # [FIX]\n",
        "try:\n",
        "    tokenizer.save_pretrained(\"./tokenizer_week2\")  # [FIX]\n",
        "except Exception as e:\n",
        "    print(f\"[FIX] tokenizer save warning: {e}\")\n",
        "\n",
        "# [FIX] Prepare metrics for aggregation\n",
        "finbert_metrics = {\n",
        "    'Model': 'FinBERT',\n",
        "    'Accuracy': float(best_pool_val),\n",
        "    'Type': 'NLP',\n",
        "    'val_metric': float(best_pool_val),\n",
        "    'train_time_sec': float(train_time_sec),\n",
        "    'params': sum(p.numel() for p in bert_model.parameters()),\n",
        "    'opt_steps': int(opt_steps),\n",
        "}\n",
        "print(f\"[FIX] FinBERT best_val={best_pool_val:.4f} at epoch={best_epoch}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "transformer"
      },
      "source": [
        "## **5. Multi-Modal Transformer Architecture**\n",
        "\n",
        "**Task 2**: Train transformer model for temporal sequence prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "transformer_model"
      },
      "outputs": [],
      "source": [
        "class MultiModalTransformer(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=256, num_heads=8, num_layers=6, num_classes=2):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.input_projection = nn.Linear(input_dim, hidden_dim)\n",
        "        self.positional_encoding = nn.Parameter(torch.randn(1000, hidden_dim))\n",
        "        \n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=hidden_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=hidden_dim * 4,\n",
        "            dropout=0.1,\n",
        "            batch_first=True\n",
        "        )\n",
        "        \n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_len, input_dim)\n",
        "        x = self.input_projection(x)\n",
        "        \n",
        "        # Add positional encoding\n",
        "        seq_len = x.size(1)\n",
        "        x = x + self.positional_encoding[:seq_len].unsqueeze(0)\n",
        "        \n",
        "        # Transformer encoding\n",
        "        x = self.transformer(x)\n",
        "        \n",
        "        # Global average pooling\n",
        "        x = x.mean(dim=1)\n",
        "        \n",
        "        # Classification\n",
        "        logits = self.classifier(x)\n",
        "        return logits\n",
        "\n",
        "# Initialize transformer\n",
        "transformer_model = MultiModalTransformer(\n",
        "    input_dim=features.shape[1],\n",
        "    hidden_dim=256,\n",
        "    num_heads=8,\n",
        "    num_layers=6\n",
        ").to(device)\n",
        "\n",
        "print(f\"âœ… Multi-modal transformer initialized with {features.shape[1]} input features\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lstm"
      },
      "source": [
        "## **6. Advanced LSTM with Attention**\n",
        "\n",
        "**Task 3**: Train bidirectional LSTM with attention mechanism"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lstm_model"
      },
      "outputs": [],
      "source": [
        "class AttentionLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=128, num_layers=2, num_classes=2):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            bidirectional=True,\n",
        "            batch_first=True,\n",
        "            dropout=0.1\n",
        "        )\n",
        "        \n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim=hidden_dim * 2,  # bidirectional\n",
        "            num_heads=4,\n",
        "            batch_first=True\n",
        "        )\n",
        "        \n",
        "        self.classifier = nn.Linear(hidden_dim * 2, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # LSTM processing\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        \n",
        "        # Self-attention\n",
        "        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
        "        \n",
        "        # Global average pooling\n",
        "        pooled = attn_out.mean(dim=1)\n",
        "        \n",
        "        # Classification\n",
        "        logits = self.classifier(pooled)\n",
        "        return logits\n",
        "\n",
        "# Initialize LSTM\n",
        "lstm_model = AttentionLSTM(\n",
        "    input_dim=features.shape[1],\n",
        "    hidden_dim=128,\n",
        "    num_layers=2\n",
        ").to(device)\n",
        "\n",
        "print(f\"âœ… Attention LSTM initialized with {features.shape[1]} input features\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training"
      },
      "source": [
        "## **7. Model Training Pipeline**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train_models"
      },
      "outputs": [],
      "source": [
        "def train_advanced_models(features, data, target_col='GME_direction_1d'):\n",
        "    \"\"\"Train all advanced models\"\"\"\n",
        "    \n",
        "    # Prepare data\n",
        "    X = features.values\n",
        "    y = data[target_col].values\n",
        "    \n",
        "    # Time series split\n",
        "    tscv = TimeSeriesSplit(n_splits=3)\n",
        "    \n",
        "    models = {\n",
        "        'transformer': transformer_model,\n",
        "        'lstm': lstm_model\n",
        "    }\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    for model_name, model in models.items():\n",
        "        print(f\"\\nðŸ”„ Training {model_name.upper()} model...\")\n",
        "        \n",
        "        # Convert to tensors\n",
        "        X_tensor = torch.FloatTensor(X).to(device)\n",
        "        y_tensor = torch.LongTensor(y).to(device)\n",
        "        \n",
        "        # Training setup\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "        \n",
        "        # [FIX] Track steps and time\n",
        "        opt_steps = 0\n",
        "        start_time = time.time()\n",
        "        \n",
        "        # Training loop\n",
        "        model.train()  # [FIX]\n",
        "        for epoch in range(5):\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward pass\n",
        "            outputs = model(X_tensor.unsqueeze(1))  # Add sequence dimension\n",
        "            loss = criterion(outputs, y_tensor)\n",
        "            \n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # [FIX]\n",
        "            optimizer.step()\n",
        "            opt_steps += 1  # [FIX]\n",
        "            \n",
        "            if epoch % 2 == 0:\n",
        "                print(f\"  [FIX] {model_name} epoch {epoch}: loss={loss.item():.4f}\")\n",
        "        \n",
        "        train_time_sec = time.time() - start_time  # [FIX]\n",
        "        \n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            predictions = model(X_tensor.unsqueeze(1))\n",
        "            pred_labels = torch.argmax(predictions, dim=1).cpu().numpy()\n",
        "            accuracy = accuracy_score(y, pred_labels)\n",
        "            \n",
        "        results[model_name] = {\n",
        "            'accuracy': accuracy,\n",
        "            'model': model,\n",
        "            'opt_steps': opt_steps,  # [FIX]\n",
        "            'train_time_sec': train_time_sec,  # [FIX]\n",
        "            'params': sum(p.numel() for p in model.parameters()),  # [FIX]\n",
        "        }\n",
        "        \n",
        "        print(f\"  âœ… {model_name.upper()} Accuracy: {accuracy:.4f}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Train all models\n",
        "print(\"\\nðŸš€ Starting advanced model training...\")\n",
        "training_results = train_advanced_models(features, data)\n",
        "print(\"\\nðŸŽ‰ Advanced model training completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ensemble"
      },
      "source": [
        "## **8. Ensemble System Training**\n",
        "\n",
        "**Task 4**: Create and train ensemble system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ensemble_training"
      },
      "outputs": [],
      "source": [
        "class AdvancedEnsemble:\n",
        "    def __init__(self, models, weights=None):\n",
        "        self.models = models\n",
        "        self.weights = weights if weights else [1/len(models)] * len(models)\n",
        "        \n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        \n",
        "        for model_name, model_info in self.models.items():\n",
        "            model = model_info['model']\n",
        "            model.eval()\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                X_tensor = torch.FloatTensor(X).to(device)\n",
        "                outputs = model(X_tensor.unsqueeze(1))\n",
        "                pred_probs = torch.softmax(outputs, dim=1).cpu().numpy()\n",
        "                predictions.append(pred_probs)\n",
        "        \n",
        "        # Weighted ensemble\n",
        "        ensemble_pred = np.zeros_like(predictions[0])\n",
        "        for i, (pred, weight) in enumerate(zip(predictions, self.weights)):\n",
        "            ensemble_pred += weight * pred\n",
        "        \n",
        "        return np.argmax(ensemble_pred, axis=1)\n",
        "\n",
        "# [FIX] Include FinBERT into ensemble inputs as optional\n",
        "if 'finbert_metrics' in globals():\n",
        "    print(\"[FIX] FinBERT metrics available; will include its score separately in results aggregation.\")\n",
        "\n",
        "# Create ensemble\n",
        "ensemble = AdvancedEnsemble(training_results)\n",
        "\n",
        "# Evaluate ensemble\n",
        "X_test = features.values\n",
        "y_test = data['GME_direction_1d'].values\n",
        "\n",
        "ensemble_preds = ensemble.predict(X_test)\n",
        "ensemble_accuracy = accuracy_score(y_test, ensemble_preds)\n",
        "\n",
        "# [FIX] Add per-model metrics row for results CSV\n",
        "for mname, minfo in training_results.items():\n",
        "    training_results[mname]['val_metric'] = minfo['accuracy']  # [FIX]\n",
        "\n",
        "print(f\"\\nðŸŽ¯ Ensemble Accuracy: {ensemble_accuracy:.4f}\")\n",
        "print(\"âœ… Advanced ensemble system completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "results"
      },
      "source": [
        "## **9. Results Summary & Model Comparison**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "results_summary"
      },
      "outputs": [],
      "source": [
        "# Compile results\n",
        "results_summary = {\n",
        "    'Model': [],\n",
        "    'Accuracy': [],\n",
        "    'Type': []\n",
        "}\n",
        "\n",
        "# [FIX] Add FinBERT result first\n",
        "if 'finbert_metrics' in globals():\n",
        "    results_summary['Model'].append(finbert_metrics['Model'])\n",
        "    results_summary['Accuracy'].append(finbert_metrics['Accuracy'])\n",
        "    results_summary['Type'].append(finbert_metrics['Type'])\n",
        "\n",
        "# Add individual model results\n",
        "for model_name, result in training_results.items():\n",
        "    results_summary['Model'].append(model_name.title())\n",
        "    results_summary['Accuracy'].append(result['accuracy'])\n",
        "    results_summary['Type'].append('Individual')\n",
        "\n",
        "# Add ensemble result\n",
        "results_summary['Model'].append('Ensemble')\n",
        "results_summary['Accuracy'].append(ensemble_accuracy)\n",
        "results_summary['Type'].append('Ensemble')\n",
        "\n",
        "# Create results DataFrame\n",
        "results_df = pd.DataFrame(results_summary)\n",
        "print(\"\\nðŸ“Š Week 2 Advanced Model Results:\")\n",
        "print(results_df)\n",
        "\n",
        "# [FIX] Save extended CSV with more metrics if available\n",
        "extended_rows = []\n",
        "if 'finbert_metrics' in globals():\n",
        "    extended_rows.append({\n",
        "        'Model': finbert_metrics['Model'],\n",
        "        'Accuracy': finbert_metrics['Accuracy'],\n",
        "        'Type': finbert_metrics['Type'],\n",
        "        'val_metric': finbert_metrics['val_metric'],\n",
        "        'train_time_sec': finbert_metrics['train_time_sec'],\n",
        "        'params': finbert_metrics['params'],\n",
        "        'opt_steps': finbert_metrics['opt_steps'],\n",
        "    })\n",
        "for mname, minfo in training_results.items():\n",
        "    extended_rows.append({\n",
        "        'Model': mname.title(),\n",
        "        'Accuracy': minfo.get('accuracy', None),\n",
        "        'Type': 'Individual',\n",
        "        'val_metric': minfo.get('val_metric', None),\n",
        "        'train_time_sec': minfo.get('train_time_sec', None),\n",
        "        'params': minfo.get('params', None),\n",
        "        'opt_steps': minfo.get('opt_steps', None),\n",
        "    })\n",
        "if extended_rows:\n",
        "    extended_df = pd.DataFrame(extended_rows)\n",
        "    extended_df.to_csv('week2_advanced_results_extended.csv', index=False)\n",
        "    print(\"[FIX] Saved extended results to week2_advanced_results_extended.csv\")\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(10, 6))\n",
        "colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D']\n",
        "bars = plt.bar(results_df['Model'], results_df['Accuracy'], color=colors)\n",
        "plt.title('Week 2 Advanced Model Performance', fontsize=16, fontweight='bold')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0, 1)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, acc in zip(bars, results_df['Accuracy']):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
        "             f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nðŸŽ‰ Week 2 Advanced Model Training Complete!\")\n",
        "print(\"\\nðŸ“‹ Next Steps:\")\n",
        "print(\"1. Save trained models\")\n",
        "print(\"2. Compare with Week 1 baseline\")\n",
        "print(\"3. Perform statistical validation\")\n",
        "print(\"4. Conduct ablation studies\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save_models"
      },
      "source": [
        "## **10. Save Trained Models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [FIX] ë³€ê²½ ìš”ì•½ ì¶œë ¥ ì…€: ë…¸íŠ¸ë¶ ì „ì²´ì—ì„œ '# [FIX]'ê°€ í¬í•¨ëœ ì¤„ì„ ìˆ˜ì§‘í•´ ë¼ì¸ ë²ˆí˜¸ì™€ í•¨ê»˜ ì¶œë ¥\n",
        "import json, re, sys\n",
        "from pathlib import Path\n",
        "\n",
        "nb_path = Path(\"colab_advanced_model_training.ipynb\")\n",
        "try:\n",
        "    import nbformat\n",
        "    nb = nbformat.read(nb_path.open('r'), as_version=4)\n",
        "    fix_lines = []\n",
        "    for ci, cell in enumerate(nb.cells):\n",
        "        if cell.cell_type != 'code':\n",
        "            continue\n",
        "        lines = cell.source.splitlines()\n",
        "        for li, line in enumerate(lines, start=1):\n",
        "            if '# [FIX]' in line:\n",
        "                fix_lines.append({\n",
        "                    'cell_index': ci,\n",
        "                    'line_in_cell': li,\n",
        "                    'text': line.strip(),\n",
        "                })\n",
        "    print(\"ë³€ê²½ ìš”ì•½ (ì…€ ì¸ë±ìŠ¤/ì…€ ë‚´ë¶€ ë¼ì¸):\")\n",
        "    for item in fix_lines:\n",
        "        print(f\" - cell {item['cell_index']} line {item['line_in_cell']}: {item['text']}\")\n",
        "    print(f\"ì´ {len(fix_lines)}ê°œ ë³€ê²½ ë¼ì¸ íƒì§€\")\n",
        "except Exception as e:\n",
        "    print(f\"ë³€ê²½ ìš”ì•½ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "model_saving"
      },
      "outputs": [],
      "source": [
        "# Save trained models\n",
        "import pickle\n",
        "\n",
        "# [FIX] Ensure opt_steps asserted before saving\n",
        "if 'finbert_metrics' in globals():\n",
        "    assert finbert_metrics['opt_steps'] > 0  # [FIX]\n",
        "\n",
        "# [FIX] Save FinBERT artifact already handled above; extend ensemble/meta saving\n",
        "\n",
        "def save_models(training_results, ensemble, results_df):\n",
        "    \"\"\"Save all trained models and results\"\"\"\n",
        "    \n",
        "    # Save individual models\n",
        "    for model_name, result in training_results.items():\n",
        "        torch.save(result['model'].state_dict(), f'{model_name}_week2.pth')\n",
        "        print(f\"âœ… Saved {model_name} model\")\n",
        "    \n",
        "    # Save ensemble with meta  # [FIX]\n",
        "    meta = {\n",
        "        'component_models': list(training_results.keys()),\n",
        "        'weights': None,\n",
        "        'created_at': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "    }\n",
        "    with open('ensemble_week2.pkl', 'wb') as f:\n",
        "        pickle.dump({'ensemble': ensemble, 'meta': meta}, f)  # [FIX]\n",
        "    print(\"âœ… Saved ensemble model with meta\")\n",
        "    \n",
        "    # Save results\n",
        "    results_df.to_csv('week2_advanced_results.csv', index=False)\n",
        "    print(\"âœ… Saved results summary\")\n",
        "    \n",
        "    # Create download links\n",
        "    from google.colab import files\n",
        "    \n",
        "    print(\"\\nðŸ“¥ Download trained models:\")\n",
        "    for model_name in training_results.keys():\n",
        "        files.download(f'{model_name}_week2.pth')\n",
        "    \n",
        "    files.download('ensemble_week2.pkl')\n",
        "    files.download('week2_advanced_results.csv')\n",
        "    if os.path.exists('week2_advanced_results_extended.csv'):\n",
        "        files.download('week2_advanced_results_extended.csv')  # [FIX]\n",
        "\n",
        "# Save all models\n",
        "save_models(training_results, ensemble, results_df)\n",
        "\n",
        "print(\"\\nðŸŽ¯ Week 2 Advanced Model Training Successfully Completed!\")\n",
        "print(\"\\nðŸ“Š Key Achievements:\")\n",
        "print(f\"- Trained {len(training_results)} advanced models\")\n",
        "print(f\"- Created ensemble system\")\n",
        "print(f\"- Best accuracy: {results_df['Accuracy'].max():.4f}\")\n",
        "print(f\"- Ensemble accuracy: {ensemble_accuracy:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
