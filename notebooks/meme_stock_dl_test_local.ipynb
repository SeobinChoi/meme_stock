{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß™ Meme Stock Deep Learning - Local Test Version\n",
    "\n",
    "## üìã Overview\n",
    "- **Purpose**: Test on MacBook with small data subset\n",
    "- **Models**: Lightweight versions of MLP, LSTM, Transformer\n",
    "- **Data**: 100 samples for quick testing\n",
    "- **Time**: ~2-5 minutes total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported\n",
      "PyTorch version: 2.2.2\n",
      "Device: CPU\n",
      "üíª Using CPU\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "üß™ Local Test Version - Small data for MacBook testing\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "print(\"‚úÖ Libraries imported\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'MPS' if torch.backends.mps.is_available() else 'CPU'}\")\n",
    "\n",
    "# Use MPS (Metal) on Mac if available\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"üéØ Using Apple Metal GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"üíª Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Create Small Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Creating test data: 100 samples\n",
      "‚úÖ Test data created:\n",
      "   Train: 60 samples\n",
      "   Val: 20 samples\n",
      "   Test: 20 samples\n"
     ]
    }
   ],
   "source": [
    "def create_test_data(n_samples=100, n_features=47, seq_length=20, seq_features=49):\n",
    "    \"\"\"Create small synthetic test data\"\"\"\n",
    "    \n",
    "    print(f\"üîß Creating test data: {n_samples} samples\")\n",
    "    \n",
    "    # Set seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Tabular data\n",
    "    X_tabular = np.random.randn(n_samples, n_features).astype(np.float32)\n",
    "    \n",
    "    # Sequence data\n",
    "    X_sequence = np.random.randn(n_samples, seq_length, seq_features).astype(np.float32)\n",
    "    \n",
    "    # Targets (with some pattern)\n",
    "    y = (\n",
    "        0.1 * X_tabular[:, 0] + \n",
    "        0.05 * X_tabular[:, 1] + \n",
    "        0.02 * np.random.randn(n_samples)\n",
    "    ).astype(np.float32)\n",
    "    \n",
    "    # Split data\n",
    "    train_size = int(0.6 * n_samples)\n",
    "    val_size = int(0.2 * n_samples)\n",
    "    \n",
    "    # Tabular splits\n",
    "    X_train_tab = X_tabular[:train_size]\n",
    "    X_val_tab = X_tabular[train_size:train_size+val_size]\n",
    "    X_test_tab = X_tabular[train_size+val_size:]\n",
    "    \n",
    "    # Sequence splits\n",
    "    X_train_seq = X_sequence[:train_size]\n",
    "    X_val_seq = X_sequence[train_size:train_size+val_size]\n",
    "    X_test_seq = X_sequence[train_size+val_size:]\n",
    "    \n",
    "    # Target splits\n",
    "    y_train = y[:train_size]\n",
    "    y_val = y[train_size:train_size+val_size]\n",
    "    y_test = y[train_size+val_size:]\n",
    "    \n",
    "    print(f\"‚úÖ Test data created:\")\n",
    "    print(f\"   Train: {len(y_train)} samples\")\n",
    "    print(f\"   Val: {len(y_val)} samples\")\n",
    "    print(f\"   Test: {len(y_test)} samples\")\n",
    "    \n",
    "    return (\n",
    "        (X_train_tab, X_val_tab, X_test_tab),\n",
    "        (X_train_seq, X_val_seq, X_test_seq),\n",
    "        (y_train, y_val, y_test)\n",
    "    )\n",
    "\n",
    "# Create test data\n",
    "tabular_data, sequence_data, targets = create_test_data(n_samples=100)\n",
    "X_train_tab, X_val_tab, X_test_tab = tabular_data\n",
    "X_train_seq, X_val_seq, X_test_seq = sequence_data\n",
    "y_train, y_val, y_test = targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Lightweight Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model architectures defined\n"
     ]
    }
   ],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    \"\"\"Lightweight MLP for testing\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dims=[64, 32]):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "class SimpleLSTM(nn.Module):\n",
    "    \"\"\"Lightweight LSTM for testing\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size=32, num_layers=1):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, hidden_size, num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "        return self.fc(last_output)\n",
    "\n",
    "\n",
    "class SimpleTransformer(nn.Module):\n",
    "    \"\"\"Lightweight Transformer for testing\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, d_model=64, nhead=4, num_layers=2):\n",
    "        super(SimpleTransformer, self).__init__()\n",
    "        \n",
    "        self.input_projection = nn.Linear(input_size, d_model)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=d_model * 2,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        \n",
    "        self.fc = nn.Linear(d_model, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_projection(x)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)  # Global pooling\n",
    "        return self.fc(x)\n",
    "\n",
    "print(\"‚úÖ Model architectures defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Quick Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_train(model, X_train, y_train, X_val, y_val, \n",
    "                epochs=20, batch_size=16, lr=0.001, model_name=\"Model\"):\n",
    "    \"\"\"Quick training for testing\"\"\"\n",
    "    \n",
    "    print(f\"\\nüéØ Training {model_name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.FloatTensor(X_train),\n",
    "        torch.FloatTensor(y_train)\n",
    "    )\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Training loop\n",
    "    train_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X).squeeze()\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"  Epoch {epoch:2d}: Loss={avg_loss:.4f}\")\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_val_tensor = torch.FloatTensor(X_val).to(device)\n",
    "        val_pred = model(X_val_tensor).cpu().numpy().flatten()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    val_corr, _ = spearmanr(y_val, val_pred)\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"‚úÖ {model_name} completed in {elapsed:.1f}s\")\n",
    "    print(f\"   Val Correlation: {val_corr:.4f}\")\n",
    "    print(f\"   Val RMSE: {val_rmse:.4f}\")\n",
    "    \n",
    "    return model, train_losses, val_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Training MLP...\n",
      "  Epoch  0: Loss=0.0328\n",
      "  Epoch  5: Loss=0.0085\n",
      "  Epoch 10: Loss=0.0048\n",
      "  Epoch 15: Loss=0.0037\n",
      "‚úÖ MLP completed in 6.9s\n",
      "   Val Correlation: 0.6617\n",
      "   Val RMSE: 0.0800\n"
     ]
    }
   ],
   "source": [
    "# Train MLP\n",
    "mlp_model = SimpleMLP(X_train_tab.shape[1])\n",
    "mlp_model, mlp_losses, mlp_corr = quick_train(\n",
    "    mlp_model, X_train_tab, y_train, X_val_tab, y_val,\n",
    "    epochs=20, model_name=\"MLP\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Training LSTM...\n",
      "  Epoch  0: Loss=0.0373\n",
      "  Epoch  5: Loss=0.0081\n",
      "  Epoch 10: Loss=0.0027\n",
      "  Epoch 15: Loss=0.0006\n",
      "‚úÖ LSTM completed in 0.5s\n",
      "   Val Correlation: 0.1308\n",
      "   Val RMSE: 0.1381\n"
     ]
    }
   ],
   "source": [
    "# Train LSTM\n",
    "lstm_model = SimpleLSTM(X_train_seq.shape[2])\n",
    "lstm_model, lstm_losses, lstm_corr = quick_train(\n",
    "    lstm_model, X_train_seq, y_train, X_val_seq, y_val,\n",
    "    epochs=20, model_name=\"LSTM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Training Transformer...\n",
      "  Epoch  0: Loss=0.0592\n",
      "  Epoch  5: Loss=0.0050\n",
      "  Epoch 10: Loss=0.0020\n",
      "  Epoch 15: Loss=0.0007\n",
      "‚úÖ Transformer completed in 4.0s\n",
      "   Val Correlation: 0.1835\n",
      "   Val RMSE: 0.1278\n"
     ]
    }
   ],
   "source": [
    "# Train Transformer\n",
    "transformer_model = SimpleTransformer(X_train_seq.shape[2])\n",
    "transformer_model, trans_losses, trans_corr = quick_train(\n",
    "    transformer_model, X_train_seq, y_train, X_val_seq, y_val,\n",
    "    epochs=20, model_name=\"Transformer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üìä TEST RESULTS\n",
      "==================================================\n",
      "MLP         : Corr=0.6226, RMSE=0.0880\n",
      "LSTM        : Corr=0.3489, RMSE=0.1302\n",
      "Transformer : Corr=0.3789, RMSE=0.1009\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    \"\"\"Evaluate model on test set\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "        y_pred = model(X_test_tensor).cpu().numpy().flatten()\n",
    "    \n",
    "    # Metrics\n",
    "    corr, _ = spearmanr(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'correlation': corr,\n",
    "        'rmse': rmse\n",
    "    }\n",
    "\n",
    "# Evaluate all models\n",
    "results = []\n",
    "\n",
    "mlp_results = evaluate_model(mlp_model, X_test_tab, y_test, 'MLP')\n",
    "results.append(mlp_results)\n",
    "\n",
    "lstm_results = evaluate_model(lstm_model, X_test_seq, y_test, 'LSTM')\n",
    "results.append(lstm_results)\n",
    "\n",
    "trans_results = evaluate_model(transformer_model, X_test_seq, y_test, 'Transformer')\n",
    "results.append(trans_results)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìä TEST RESULTS\")\n",
    "print(\"=\"*50)\n",
    "for result in results:\n",
    "    print(f\"{result['model']:12s}: Corr={result['correlation']:.4f}, RMSE={result['rmse']:.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Load Real Data Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Testing real data loading...\n",
      "‚úÖ Found data with timestamp: 20250814_031335\n",
      "‚úÖ Loaded train data: (50, 47)\n",
      "‚úÖ Loaded sequences: ['AMC_sequences', 'AMC_targets_1d', 'AMC_targets_5d']...\n"
     ]
    }
   ],
   "source": [
    "def test_real_data_loading():\n",
    "    \"\"\"Test loading real data files\"\"\"\n",
    "    \n",
    "    print(\"\\nüîç Testing real data loading...\")\n",
    "    \n",
    "    try:\n",
    "        # Try to load metadata\n",
    "        import glob\n",
    "        metadata_files = glob.glob('../data/colab_datasets/*metadata*.json')\n",
    "        \n",
    "        if metadata_files:\n",
    "            with open(metadata_files[0], 'r') as f:\n",
    "                metadata = json.load(f)\n",
    "            \n",
    "            timestamp = metadata['timestamp']\n",
    "            print(f\"‚úÖ Found data with timestamp: {timestamp}\")\n",
    "            \n",
    "            # Try loading CSVs\n",
    "            train_df = pd.read_csv(f'../data/colab_datasets/tabular_train_{timestamp}.csv', nrows=50)\n",
    "            print(f\"‚úÖ Loaded train data: {train_df.shape}\")\n",
    "            \n",
    "            # Try loading NPZ\n",
    "            sequences = np.load(f'../data/colab_datasets/sequences_{timestamp}.npz')\n",
    "            print(f\"‚úÖ Loaded sequences: {list(sequences.keys())[:3]}...\")\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No metadata file found\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading real data: {e}\")\n",
    "        return False\n",
    "\n",
    "# Test real data loading\n",
    "real_data_ok = test_real_data_loading()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üéØ LOCAL TEST SUMMARY\n",
      "==================================================\n",
      "\n",
      "‚úÖ Models Tested:\n",
      "   - MLP: Working\n",
      "   - LSTM: Working\n",
      "   - Transformer: Working\n",
      "\n",
      "‚úÖ Device:\n",
      "   - Using: cpu\n",
      "\n",
      "‚úÖ Data:\n",
      "   - Synthetic test data: Working\n",
      "   - Real data loading: Working\n",
      "\n",
      "==================================================\n",
      "üöÄ Ready for Colab A100 deployment!\n",
      "==================================================\n",
      "\n",
      "‚úÖ Test results saved to local_test_results.json\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üéØ LOCAL TEST SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\n‚úÖ Models Tested:\")\n",
    "print(\"   - MLP: Working\")\n",
    "print(\"   - LSTM: Working\")\n",
    "print(\"   - Transformer: Working\")\n",
    "\n",
    "print(\"\\n‚úÖ Device:\")\n",
    "print(f\"   - Using: {device}\")\n",
    "if device.type == \"mps\":\n",
    "    print(\"   - Apple Metal GPU acceleration enabled\")\n",
    "\n",
    "print(\"\\n‚úÖ Data:\")\n",
    "print(\"   - Synthetic test data: Working\")\n",
    "if real_data_ok:\n",
    "    print(\"   - Real data loading: Working\")\n",
    "else:\n",
    "    print(\"   - Real data loading: Not available (expected on local)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üöÄ Ready for Colab A100 deployment!\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Save test results\n",
    "test_summary = {\n",
    "    'timestamp': datetime.now().strftime('%Y%m%d_%H%M%S'),\n",
    "    'device': str(device),\n",
    "    'models_tested': ['MLP', 'LSTM', 'Transformer'],\n",
    "    'test_results': results,\n",
    "    'real_data_available': real_data_ok,\n",
    "    'status': 'PASSED'\n",
    "}\n",
    "\n",
    "with open('local_test_results.json', 'w') as f:\n",
    "    json.dump(test_summary, f, indent=2, default=str)\n",
    "\n",
    "print(\"\\n‚úÖ Test results saved to local_test_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meme_stock_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
