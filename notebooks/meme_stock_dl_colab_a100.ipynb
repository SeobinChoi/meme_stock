{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Meme Stock Deep Learning - Colab A100 Optimized\n",
    "\n",
    "## üìã Overview\n",
    "- **Target**: IC ‚â• 0.05 for 1-day return prediction\n",
    "- **Models**: MLP, LSTM, Transformer, Ensemble\n",
    "- **GPU**: A100 40GB optimized with FP16\n",
    "- **Data**: 5.4K samples, 47 tabular + 49 sequence features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "üöÄ Meme Stock Deep Learning Pipeline - A100 GPU Optimized\n",
    "Fixed version with robust data loading and error handling\n",
    "\"\"\"\n",
    "\n",
    "# Install required packages\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_packages():\n",
    "    \"\"\"Install required packages for A100 GPU\"\"\"\n",
    "    packages = [\n",
    "        \"pytorch-tabnet\",\n",
    "        \"transformers\",\n",
    "        \"optuna\",\n",
    "        \"plotly\",\n",
    "        \"scikit-learn\",\n",
    "        \"scipy\"\n",
    "    ]\n",
    "    \n",
    "    for package in packages:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "            print(f\"‚úÖ {package} installed\")\n",
    "        except:\n",
    "            print(f\"‚ö†Ô∏è {package} may already be installed\")\n",
    "\n",
    "# Only install in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"üéØ Running in Google Colab\")\n",
    "    install_packages()\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"üíª Running locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# ML/DL libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "# TabNet\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ GPU Setup & Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def load_all_data():\n    \"\"\"Load all data with robust error handling\"\"\"\n    \n    # Find metadata file\n    metadata_files = glob.glob('*metadata*.json')\n    if not metadata_files:\n        # Try colab_datasets directory\n        metadata_files = glob.glob('data/colab_datasets/*metadata*.json')\n    \n    if not metadata_files:\n        raise FileNotFoundError(\"‚ùå No metadata file found!\")\n    \n    # Load metadata\n    with open(metadata_files[0], 'r') as f:\n        metadata = json.load(f)\n    \n    timestamp = metadata['timestamp']\n    print(f\"üìä Loading data with timestamp: {timestamp}\")\n    \n    # Try different paths\n    paths_to_try = ['', 'data/colab_datasets/']\n    \n    for path in paths_to_try:\n        try:\n            # Load tabular data\n            train_df = pd.read_csv(f'{path}tabular_train_{timestamp}.csv')\n            val_df = pd.read_csv(f'{path}tabular_val_{timestamp}.csv')\n            test_df = pd.read_csv(f'{path}tabular_test_{timestamp}.csv')\n            \n            # Load sequence data with allow_pickle=True to handle object arrays\n            sequences_data = np.load(f'{path}sequences_{timestamp}.npz', allow_pickle=True)\n            \n            print(f\"‚úÖ Data loaded from {path if path else 'current directory'}\")\n            break\n        except FileNotFoundError:\n            continue\n    else:\n        raise FileNotFoundError(\"‚ùå Could not find data files!\")\n    \n    # Convert dates\n    for df in [train_df, val_df, test_df]:\n        df['date'] = pd.to_datetime(df['date'])\n    \n    print(f\"\\nüìà Data loaded successfully:\")\n    print(f\"   Train: {len(train_df):,} samples\")\n    print(f\"   Val: {len(val_df):,} samples\")\n    print(f\"   Test: {len(test_df):,} samples\")\n    print(f\"   Features: {len(metadata['tabular_features'])}\")\n    print(f\"   Tickers: {metadata['tickers']}\")\n    \n    return train_df, val_df, test_df, sequences_data, metadata\n\n# Load data\ntrain_df, val_df, test_df, sequences_data, metadata = load_all_data()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def prepare_sequence_data_fixed(sequences_data, metadata):\n    \"\"\"Prepare sequence data with robust error handling\"\"\"\n    \n    print(\"üîÑ Preparing sequence data...\")\n    \n    all_sequences = []\n    all_targets = []\n    all_dates = []\n    target_features = 48  # Most tickers have 48 features\n    \n    # Process each ticker\n    for ticker in metadata['tickers']:\n        seq_key = f'{ticker}_sequences'\n        \n        if seq_key in sequences_data:\n            sequences = sequences_data[seq_key]\n            targets = sequences_data[f'{ticker}_targets_1d']\n            dates = sequences_data[f'{ticker}_dates']\n            \n            # Handle object dtype (string columns)\n            if sequences.dtype == object:\n                print(f\"   ‚ö†Ô∏è {ticker}: Cleaning object dtype...\")\n                \n                # Find numeric columns only\n                numeric_cols = []\n                for i in range(sequences.shape[2]):\n                    try:\n                        test_col = sequences[:, :, i].astype(np.float32)\n                        if np.any(np.isfinite(test_col)):\n                            numeric_cols.append(i)\n                    except:\n                        continue\n                \n                if numeric_cols:\n                    sequences = sequences[:, :, numeric_cols].astype(np.float32)\n                else:\n                    print(f\"   ‚ùå {ticker}: No numeric columns, skipping\")\n                    continue\n            else:\n                sequences = sequences.astype(np.float32)\n            \n            # Clean NaN/Inf\n            sequences = np.nan_to_num(sequences, nan=0.0, posinf=0.0, neginf=0.0)\n            \n            # Fix AMC (or any ticker with 47 features) by adding a zero column\n            if sequences.shape[2] == 47:\n                print(f\"   üîß {ticker}: Adding 1 zero column (47 -> 48 features)\")\n                zero_column = np.zeros((sequences.shape[0], sequences.shape[1], 1), dtype=np.float32)\n                sequences = np.concatenate([sequences, zero_column], axis=2)\n            \n            all_sequences.append(sequences)\n            all_targets.extend(targets)\n            all_dates.extend(dates)\n            \n            print(f\"   ‚úÖ {ticker}: {sequences.shape}\")\n    \n    if not all_sequences:\n        print(\"‚ùå No valid sequences found\")\n        return None, None, None\n    \n    # Stack all sequences (now all have 48 features)\n    X_seq = np.vstack(all_sequences).astype(np.float32)\n    y_seq = np.array(all_targets, dtype=np.float32)\n    \n    print(f\"\\n‚úÖ Sequence data prepared:\")\n    print(f\"   X_seq: {X_seq.shape}\")\n    print(f\"   y_seq: {y_seq.shape}\")\n    \n    # Split by date\n    dates_array = np.array([pd.to_datetime(d) for d in all_dates])\n    \n    train_end = pd.to_datetime('2023-02-02')\n    val_end = pd.to_datetime('2023-07-15')\n    \n    train_mask = dates_array <= train_end\n    val_mask = (dates_array > train_end) & (dates_array <= val_end)\n    test_mask = dates_array > val_end\n    \n    X_train_seq = X_seq[train_mask]\n    X_val_seq = X_seq[val_mask]\n    X_test_seq = X_seq[test_mask]\n    \n    y_train_seq = y_seq[train_mask]\n    y_val_seq = y_seq[val_mask]\n    y_test_seq = y_seq[test_mask]\n    \n    print(f\"\\nüìä Sequence data split:\")\n    print(f\"   Train: {X_train_seq.shape}\")\n    print(f\"   Val: {X_val_seq.shape}\")\n    print(f\"   Test: {X_test_seq.shape}\")\n    \n    return (X_train_seq, X_val_seq, X_test_seq,\n            y_train_seq, y_val_seq, y_test_seq)\n\n# Prepare sequence data\ntry:\n    # Load with allow_pickle=True to handle object arrays\n    sequences_data_dict = dict(sequences_data)  # Convert to dict if needed\n    seq_data = prepare_sequence_data_fixed(sequences_data_dict, metadata)\n    if seq_data[0] is not None:\n        X_train_seq, X_val_seq, X_test_seq, y_train_seq, y_val_seq, y_test_seq = seq_data\n        USE_SEQUENCE_MODELS = True\n        print(\"‚úÖ Sequence models enabled\")\n    else:\n        USE_SEQUENCE_MODELS = False\n        print(\"‚ö†Ô∏è Sequence models disabled\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è Sequence preparation failed: {e}\")\n    USE_SEQUENCE_MODELS = False"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_data():\n",
    "    \"\"\"Load all data with robust error handling\"\"\"\n",
    "    \n",
    "    # Find metadata file\n",
    "    metadata_files = glob.glob('*metadata*.json')\n",
    "    if not metadata_files:\n",
    "        # Try colab_datasets directory\n",
    "        metadata_files = glob.glob('data/colab_datasets/*metadata*.json')\n",
    "    \n",
    "    if not metadata_files:\n",
    "        raise FileNotFoundError(\"‚ùå No metadata file found!\")\n",
    "    \n",
    "    # Load metadata\n",
    "    with open(metadata_files[0], 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    timestamp = metadata['timestamp']\n",
    "    print(f\"üìä Loading data with timestamp: {timestamp}\")\n",
    "    \n",
    "    # Try different paths\n",
    "    paths_to_try = ['', 'data/colab_datasets/']\n",
    "    \n",
    "    for path in paths_to_try:\n",
    "        try:\n",
    "            # Load tabular data\n",
    "            train_df = pd.read_csv(f'{path}tabular_train_{timestamp}.csv')\n",
    "            val_df = pd.read_csv(f'{path}tabular_val_{timestamp}.csv')\n",
    "            test_df = pd.read_csv(f'{path}tabular_test_{timestamp}.csv')\n",
    "            \n",
    "            # Load sequence data\n",
    "            sequences_data = np.load(f'{path}sequences_{timestamp}.npz')\n",
    "            \n",
    "            print(f\"‚úÖ Data loaded from {path if path else 'current directory'}\")\n",
    "            break\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "    else:\n",
    "        raise FileNotFoundError(\"‚ùå Could not find data files!\")\n",
    "    \n",
    "    # Convert dates\n",
    "    for df in [train_df, val_df, test_df]:\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    print(f\"\\nüìà Data loaded successfully:\")\n",
    "    print(f\"   Train: {len(train_df):,} samples\")\n",
    "    print(f\"   Val: {len(val_df):,} samples\")\n",
    "    print(f\"   Test: {len(test_df):,} samples\")\n",
    "    print(f\"   Features: {len(metadata['tabular_features'])}\")\n",
    "    print(f\"   Tickers: {metadata['tickers']}\")\n",
    "    \n",
    "    return train_df, val_df, test_df, sequences_data, metadata\n",
    "\n",
    "# Load data\n",
    "train_df, val_df, test_df, sequences_data, metadata = load_all_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tabular_data(train_df, val_df, test_df, target='y1d'):\n",
    "    \"\"\"Prepare tabular data for modeling\"\"\"\n",
    "    \n",
    "    # Feature columns (exclude metadata and targets)\n",
    "    exclude_cols = ['date', 'ticker', 'ticker_type', 'y1d', 'y5d', \n",
    "                   'alpha_1d', 'alpha_5d', 'direction_1d', 'direction_5d']\n",
    "    feature_cols = [col for col in train_df.columns if col not in exclude_cols]\n",
    "    \n",
    "    print(f\"üìä Using {len(feature_cols)} features\")\n",
    "    \n",
    "    # Prepare features\n",
    "    X_train = train_df[feature_cols].fillna(0).values.astype(np.float32)\n",
    "    X_val = val_df[feature_cols].fillna(0).values.astype(np.float32)\n",
    "    X_test = test_df[feature_cols].fillna(0).values.astype(np.float32)\n",
    "    \n",
    "    # Prepare targets\n",
    "    y_train = train_df[target].values.astype(np.float32)\n",
    "    y_val = val_df[target].values.astype(np.float32)\n",
    "    y_test = test_df[target].values.astype(np.float32)\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = RobustScaler()  # Robust to outliers\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    print(f\"‚úÖ Tabular data prepared\")\n",
    "    print(f\"   X_train: {X_train_scaled.shape}\")\n",
    "    print(f\"   y_train: {y_train.shape}\")\n",
    "    \n",
    "    return (X_train_scaled, X_val_scaled, X_test_scaled,\n",
    "            y_train, y_val, y_test, feature_cols, scaler)\n",
    "\n",
    "# Prepare tabular data\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, feature_cols, scaler = prepare_tabular_data(\n",
    "    train_df, val_df, test_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence_data_fixed(sequences_data, metadata):\n",
    "    \"\"\"Prepare sequence data with robust error handling\"\"\"\n",
    "    \n",
    "    print(\"üîÑ Preparing sequence data...\")\n",
    "    \n",
    "    all_sequences = []\n",
    "    all_targets = []\n",
    "    all_dates = []\n",
    "    \n",
    "    # Process each ticker\n",
    "    for ticker in metadata['tickers']:\n",
    "        seq_key = f'{ticker}_sequences'\n",
    "        \n",
    "        if seq_key in sequences_data:\n",
    "            sequences = sequences_data[seq_key]\n",
    "            targets = sequences_data[f'{ticker}_targets_1d']\n",
    "            dates = sequences_data[f'{ticker}_dates']\n",
    "            \n",
    "            # Handle object dtype (string columns)\n",
    "            if sequences.dtype == object:\n",
    "                print(f\"   ‚ö†Ô∏è {ticker}: Cleaning object dtype...\")\n",
    "                \n",
    "                # Find numeric columns only\n",
    "                numeric_cols = []\n",
    "                for i in range(sequences.shape[2]):\n",
    "                    try:\n",
    "                        test_col = sequences[:, :, i].astype(np.float32)\n",
    "                        if np.any(np.isfinite(test_col)):\n",
    "                            numeric_cols.append(i)\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                if numeric_cols:\n",
    "                    sequences = sequences[:, :, numeric_cols].astype(np.float32)\n",
    "                else:\n",
    "                    print(f\"   ‚ùå {ticker}: No numeric columns, skipping\")\n",
    "                    continue\n",
    "            else:\n",
    "                sequences = sequences.astype(np.float32)\n",
    "            \n",
    "            # Clean NaN/Inf\n",
    "            sequences = np.nan_to_num(sequences, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            \n",
    "            all_sequences.append(sequences)\n",
    "            all_targets.extend(targets)\n",
    "            all_dates.extend(dates)\n",
    "            \n",
    "            print(f\"   ‚úÖ {ticker}: {sequences.shape}\")\n",
    "    \n",
    "    if not all_sequences:\n",
    "        print(\"‚ùå No valid sequences found\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Stack all sequences\n",
    "    X_seq = np.vstack(all_sequences).astype(np.float32)\n",
    "    y_seq = np.array(all_targets, dtype=np.float32)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Sequence data prepared:\")\n",
    "    print(f\"   X_seq: {X_seq.shape}\")\n",
    "    print(f\"   y_seq: {y_seq.shape}\")\n",
    "    \n",
    "    # Split by date\n",
    "    dates_array = np.array([pd.to_datetime(d) for d in all_dates])\n",
    "    \n",
    "    train_end = pd.to_datetime('2023-02-02')\n",
    "    val_end = pd.to_datetime('2023-07-15')\n",
    "    \n",
    "    train_mask = dates_array <= train_end\n",
    "    val_mask = (dates_array > train_end) & (dates_array <= val_end)\n",
    "    test_mask = dates_array > val_end\n",
    "    \n",
    "    X_train_seq = X_seq[train_mask]\n",
    "    X_val_seq = X_seq[val_mask]\n",
    "    X_test_seq = X_seq[test_mask]\n",
    "    \n",
    "    y_train_seq = y_seq[train_mask]\n",
    "    y_val_seq = y_seq[val_mask]\n",
    "    y_test_seq = y_seq[test_mask]\n",
    "    \n",
    "    print(f\"\\nüìä Sequence data split:\")\n",
    "    print(f\"   Train: {X_train_seq.shape}\")\n",
    "    print(f\"   Val: {X_val_seq.shape}\")\n",
    "    print(f\"   Test: {X_test_seq.shape}\")\n",
    "    \n",
    "    return (X_train_seq, X_val_seq, X_test_seq,\n",
    "            y_train_seq, y_val_seq, y_test_seq)\n",
    "\n",
    "# Prepare sequence data\n",
    "try:\n",
    "    seq_data = prepare_sequence_data_fixed(sequences_data, metadata)\n",
    "    if seq_data[0] is not None:\n",
    "        X_train_seq, X_val_seq, X_test_seq, y_train_seq, y_val_seq, y_test_seq = seq_data\n",
    "        USE_SEQUENCE_MODELS = True\n",
    "        print(\"‚úÖ Sequence models enabled\")\n",
    "    else:\n",
    "        USE_SEQUENCE_MODELS = False\n",
    "        print(\"‚ö†Ô∏è Sequence models disabled\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Sequence preparation failed: {e}\")\n",
    "    USE_SEQUENCE_MODELS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepMLP(nn.Module):\n",
    "    \"\"\"Deep MLP with BatchNorm and Dropout\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dims=[512, 256, 128, 64], dropout=0.3):\n",
    "        super(DeepMLP, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    \"\"\"LSTM for sequence modeling\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size=256, num_layers=2, dropout=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, hidden_size, num_layers,\n",
    "            batch_first=True, dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # Use last timestep\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "        return self.fc(last_output)\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    \"\"\"Transformer for sequence modeling\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, d_model=256, nhead=8, num_layers=4, dropout=0.2):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_projection = nn.Linear(input_size, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoder = nn.Parameter(torch.randn(1, 20, d_model))\n",
    "        \n",
    "        # Transformer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        \n",
    "        # Output layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Project input\n",
    "        x = self.input_projection(x)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = x + self.pos_encoder[:, :x.size(1), :]\n",
    "        \n",
    "        # Transformer encoding\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # Global pooling\n",
    "        x = x.mean(dim=1)\n",
    "        \n",
    "        return self.fc(x)\n",
    "\n",
    "print(\"‚úÖ Model architectures defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ic_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculate Information Coefficient metrics\"\"\"\n",
    "    \n",
    "    # Remove NaN values\n",
    "    mask = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "    if mask.sum() < 2:\n",
    "        return {'ic': 0, 'rank_ic': 0, 'hit_rate': 0.5}\n",
    "    \n",
    "    y_true_clean = y_true[mask]\n",
    "    y_pred_clean = y_pred[mask]\n",
    "    \n",
    "    # Calculate correlations\n",
    "    ic, _ = pearsonr(y_pred_clean, y_true_clean)\n",
    "    rank_ic, _ = spearmanr(y_pred_clean, y_true_clean)\n",
    "    \n",
    "    # Hit rate (directional accuracy)\n",
    "    hit_rate = np.mean(np.sign(y_pred_clean) == np.sign(y_true_clean))\n",
    "    \n",
    "    return {\n",
    "        'ic': ic if not np.isnan(ic) else 0,\n",
    "        'rank_ic': rank_ic if not np.isnan(rank_ic) else 0,\n",
    "        'hit_rate': hit_rate\n",
    "    }\n",
    "\n",
    "\n",
    "def train_model_with_amp(model, train_loader, val_loader, \n",
    "                         epochs=100, lr=0.001, device='cuda',\n",
    "                         model_name=\"Model\"):\n",
    "    \"\"\"Train model with mixed precision (AMP) for A100\"\"\"\n",
    "    \n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)\n",
    "    \n",
    "    # Mixed precision training\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    train_losses = []\n",
    "    val_ics = []\n",
    "    best_ic = -float('inf')\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(f\"\\nüéØ Training {model_name}...\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Mixed precision forward pass\n",
    "            with autocast():\n",
    "                outputs = model(batch_X).squeeze()\n",
    "                loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            # Backward pass\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_predictions = []\n",
    "        val_actuals = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X = batch_X.to(device)\n",
    "                outputs = model(batch_X).squeeze()\n",
    "                \n",
    "                val_predictions.extend(outputs.cpu().numpy())\n",
    "                val_actuals.extend(batch_y.numpy())\n",
    "        \n",
    "        # Calculate validation IC\n",
    "        val_metrics = calculate_ic_metrics(\n",
    "            np.array(val_actuals), \n",
    "            np.array(val_predictions)\n",
    "        )\n",
    "        val_ic = val_metrics['rank_ic']\n",
    "        \n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "        val_ics.append(val_ic)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_ic)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_ic > best_ic:\n",
    "            best_ic = val_ic\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Print progress\n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"  Epoch {epoch:3d}: Loss={train_loss/len(train_loader):.4f}, \"\n",
    "                  f\"Val IC={val_ic:.4f}, Best IC={best_ic:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= 30:\n",
    "            print(f\"  Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(f\"‚úÖ {model_name} training completed. Best IC: {best_ic:.4f}\")\n",
    "    \n",
    "    return model, train_losses, val_ics, best_ic\n",
    "\n",
    "print(\"‚úÖ Training functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "BATCH_SIZE = 256 if device.type == 'cuda' else 64\n",
    "EPOCHS = 200\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "print(f\"üìä Training Configuration:\")\n",
    "print(f\"   Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"   Epochs: {EPOCHS}\")\n",
    "print(f\"   Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"   Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data loaders\n",
    "train_dataset = TensorDataset(\n",
    "    torch.FloatTensor(X_train),\n",
    "    torch.FloatTensor(y_train)\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(\n",
    "    torch.FloatTensor(X_val),\n",
    "    torch.FloatTensor(y_val)\n",
    ")\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"‚úÖ Data loaders created\")\n",
    "print(f\"   Train batches: {len(train_loader)}\")\n",
    "print(f\"   Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train MLP\n",
    "mlp_model = DeepMLP(X_train.shape[1], hidden_dims=[512, 256, 128, 64])\n",
    "mlp_model, mlp_losses, mlp_ics, mlp_best_ic = train_model_with_amp(\n",
    "    mlp_model, train_loader, val_loader,\n",
    "    epochs=EPOCHS, lr=LEARNING_RATE, device=device,\n",
    "    model_name=\"MLP\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LSTM (if sequence data available)\n",
    "if USE_SEQUENCE_MODELS:\n",
    "    # Prepare sequence data loaders\n",
    "    train_seq_dataset = TensorDataset(\n",
    "        torch.FloatTensor(X_train_seq),\n",
    "        torch.FloatTensor(y_train_seq)\n",
    "    )\n",
    "    train_seq_loader = DataLoader(train_seq_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    val_seq_dataset = TensorDataset(\n",
    "        torch.FloatTensor(X_val_seq),\n",
    "        torch.FloatTensor(y_val_seq)\n",
    "    )\n",
    "    val_seq_loader = DataLoader(val_seq_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # Train LSTM\n",
    "    lstm_model = LSTMModel(X_train_seq.shape[2], hidden_size=256, num_layers=2)\n",
    "    lstm_model, lstm_losses, lstm_ics, lstm_best_ic = train_model_with_amp(\n",
    "        lstm_model, train_seq_loader, val_seq_loader,\n",
    "        epochs=EPOCHS, lr=LEARNING_RATE, device=device,\n",
    "        model_name=\"LSTM\"\n",
    "    )\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è LSTM training skipped (no sequence data)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Transformer (if sequence data available)\n",
    "if USE_SEQUENCE_MODELS:\n",
    "    # Train Transformer\n",
    "    transformer_model = TransformerModel(\n",
    "        X_train_seq.shape[2], d_model=256, nhead=8, num_layers=4\n",
    "    )\n",
    "    transformer_model, trans_losses, trans_ics, trans_best_ic = train_model_with_amp(\n",
    "        transformer_model, train_seq_loader, val_seq_loader,\n",
    "        epochs=EPOCHS, lr=LEARNING_RATE*0.5, device=device,\n",
    "        model_name=\"Transformer\"\n",
    "    )\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Transformer training skipped (no sequence data)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train TabNet\nprint(\"\\nüéØ Training TabNet...\")\n\ntabnet_model = TabNetRegressor(\n    n_d=32, n_a=32,\n    n_steps=5,\n    gamma=1.5,\n    cat_idxs=[],\n    cat_dims=[],\n    cat_emb_dim=1,\n    lambda_sparse=1e-4,\n    momentum=0.3,\n    clip_value=2.0,\n    optimizer_fn=torch.optim.AdamW,\n    optimizer_params=dict(lr=0.002, weight_decay=1e-5),\n    scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n    scheduler_params=dict(patience=10, factor=0.5),\n    mask_type=\"entmax\",\n    seed=42,\n    verbose=0,\n    device_name='cuda' if device.type == 'cuda' else 'cpu'\n)\n\n# Reshape targets to 2D for TabNet (required format)\ny_train_2d = y_train.reshape(-1, 1)\ny_val_2d = y_val.reshape(-1, 1)\n\n# Train TabNet\ntabnet_model.fit(\n    X_train, y_train_2d,\n    eval_set=[(X_val, y_val_2d)],\n    eval_metric=['rmse'],\n    max_epochs=100,\n    patience=20,\n    batch_size=256 if device.type == 'cuda' else 64\n)\n\n# Evaluate TabNet\ntabnet_pred_val = tabnet_model.predict(X_val).flatten()  # Flatten back to 1D\ntabnet_metrics = calculate_ic_metrics(y_val, tabnet_pred_val)\nprint(f\"‚úÖ TabNet training completed. Val IC: {tabnet_metrics['rank_ic']:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def evaluate_model(model, X_test, y_test, model_name, device, is_sequence=False):\n    \"\"\"Evaluate model on test set\"\"\"\n    \n    if hasattr(model, 'predict'):\n        # TabNet\n        y_pred = model.predict(X_test)\n        # Ensure predictions are 1D\n        if len(y_pred.shape) > 1:\n            y_pred = y_pred.flatten()\n    else:\n        # PyTorch model\n        model.eval()\n        with torch.no_grad():\n            X_tensor = torch.FloatTensor(X_test).to(device)\n            y_pred = model(X_tensor).cpu().numpy().flatten()\n    \n    # Ensure y_test is also 1D\n    if len(y_test.shape) > 1:\n        y_test = y_test.flatten()\n    \n    # Calculate metrics\n    metrics = calculate_ic_metrics(y_test, y_pred)\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n    mae = mean_absolute_error(y_test, y_pred)\n    \n    results = {\n        'model': model_name,\n        'ic': metrics['ic'],\n        'rank_ic': metrics['rank_ic'],\n        'hit_rate': metrics['hit_rate'],\n        'rmse': rmse,\n        'mae': mae\n    }\n    \n    return results, y_pred\n\n# Evaluate all models\nresults = []\n\n# MLP\nmlp_results, mlp_predictions = evaluate_model(\n    mlp_model, X_test, y_test, 'MLP', device\n)\nresults.append(mlp_results)\n\n# TabNet\ntabnet_results, tabnet_predictions = evaluate_model(\n    tabnet_model, X_test, y_test, 'TabNet', device\n)\nresults.append(tabnet_results)\n\n# LSTM and Transformer\nif USE_SEQUENCE_MODELS:\n    lstm_results, lstm_predictions = evaluate_model(\n        lstm_model, X_test_seq, y_test_seq, 'LSTM', device, is_sequence=True\n    )\n    results.append(lstm_results)\n    \n    trans_results, trans_predictions = evaluate_model(\n        transformer_model, X_test_seq, y_test_seq, 'Transformer', device, is_sequence=True\n    )\n    results.append(trans_results)\n\n# Display results\nresults_df = pd.DataFrame(results)\nresults_df = results_df.sort_values('rank_ic', ascending=False)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"üìä MODEL EVALUATION RESULTS\")\nprint(\"=\"*60)\nprint(results_df.to_string(index=False))\nprint(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ensemble_predictions(models_predictions, weights=None):\n",
    "    \"\"\"Create ensemble predictions\"\"\"\n",
    "    \n",
    "    if weights is None:\n",
    "        # Equal weights\n",
    "        weights = [1/len(models_predictions)] * len(models_predictions)\n",
    "    \n",
    "    # Weighted average\n",
    "    ensemble_pred = np.zeros_like(models_predictions[0])\n",
    "    for pred, weight in zip(models_predictions, weights):\n",
    "        ensemble_pred += pred * weight\n",
    "    \n",
    "    return ensemble_pred\n",
    "\n",
    "# Create ensemble\n",
    "if USE_SEQUENCE_MODELS:\n",
    "    # For sequence models, we need to align predictions\n",
    "    # Use only tabular models for now\n",
    "    ensemble_predictions = create_ensemble_predictions(\n",
    "        [mlp_predictions, tabnet_predictions],\n",
    "        weights=[0.6, 0.4]  # Give more weight to MLP\n",
    "    )\n",
    "else:\n",
    "    ensemble_predictions = create_ensemble_predictions(\n",
    "        [mlp_predictions, tabnet_predictions],\n",
    "        weights=[0.6, 0.4]\n",
    "    )\n",
    "\n",
    "# Evaluate ensemble\n",
    "ensemble_metrics = calculate_ic_metrics(y_test, ensemble_predictions)\n",
    "ensemble_rmse = np.sqrt(mean_squared_error(y_test, ensemble_predictions))\n",
    "\n",
    "print(\"\\nüéØ ENSEMBLE RESULTS:\")\n",
    "print(f\"   IC: {ensemble_metrics['ic']:.4f}\")\n",
    "print(f\"   Rank IC: {ensemble_metrics['rank_ic']:.4f}\")\n",
    "print(f\"   Hit Rate: {ensemble_metrics['hit_rate']:.3%}\")\n",
    "print(f\"   RMSE: {ensemble_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîü Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# MLP training\n",
    "axes[0].plot(mlp_losses, label='Train Loss', alpha=0.7)\n",
    "axes[0].set_title('MLP Training Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# IC evolution\n",
    "axes[1].plot(mlp_ics, label='MLP', alpha=0.7)\n",
    "if USE_SEQUENCE_MODELS:\n",
    "    axes[1].plot(lstm_ics, label='LSTM', alpha=0.7)\n",
    "    axes[1].plot(trans_ics, label='Transformer', alpha=0.7)\n",
    "axes[1].axhline(y=0.03, color='r', linestyle='--', label='Target IC')\n",
    "axes[1].set_title('Validation IC Evolution')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Rank IC')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions vs Actual scatter plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# MLP predictions\n",
    "axes[0].scatter(y_test, mlp_predictions, alpha=0.5, s=10)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[0].set_title(f'MLP Predictions (IC={mlp_results[\"rank_ic\"]:.4f})')\n",
    "axes[0].set_xlabel('Actual Returns')\n",
    "axes[0].set_ylabel('Predicted Returns')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Ensemble predictions\n",
    "axes[1].scatter(y_test, ensemble_predictions, alpha=0.5, s=10)\n",
    "axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[1].set_title(f'Ensemble Predictions (IC={ensemble_metrics[\"rank_ic\"]:.4f})')\n",
    "axes[1].set_xlabel('Actual Returns')\n",
    "axes[1].set_ylabel('Predicted Returns')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üèÜ FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Best single model\n",
    "best_model = results_df.iloc[0]\n",
    "print(f\"\\nüìå Best Single Model: {best_model['model']}\")\n",
    "print(f\"   Rank IC: {best_model['rank_ic']:.4f}\")\n",
    "print(f\"   IC: {best_model['ic']:.4f}\")\n",
    "print(f\"   Hit Rate: {best_model['hit_rate']:.3%}\")\n",
    "print(f\"   RMSE: {best_model['rmse']:.4f}\")\n",
    "\n",
    "# Ensemble performance\n",
    "print(f\"\\nüéØ Ensemble Model:\")\n",
    "print(f\"   Rank IC: {ensemble_metrics['rank_ic']:.4f}\")\n",
    "print(f\"   IC: {ensemble_metrics['ic']:.4f}\")\n",
    "print(f\"   Hit Rate: {ensemble_metrics['hit_rate']:.3%}\")\n",
    "print(f\"   RMSE: {ensemble_rmse:.4f}\")\n",
    "\n",
    "# Go/No-Go decision\n",
    "best_ic = max(best_model['rank_ic'], ensemble_metrics['rank_ic'])\n",
    "meets_threshold = best_ic >= 0.03\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if meets_threshold:\n",
    "    print(\"‚úÖ GO DECISION: Model meets success criteria (IC ‚â• 0.03)\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è CONTINUE: Current best IC={best_ic:.4f} < 0.03 threshold\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save results\n",
    "results_dict = {\n",
    "    'timestamp': datetime.now().strftime('%Y%m%d_%H%M%S'),\n",
    "    'models': results,\n",
    "    'ensemble': {\n",
    "        'ic': ensemble_metrics['ic'],\n",
    "        'rank_ic': ensemble_metrics['rank_ic'],\n",
    "        'hit_rate': ensemble_metrics['hit_rate'],\n",
    "        'rmse': ensemble_rmse\n",
    "    },\n",
    "    'best_ic': best_ic,\n",
    "    'meets_threshold': meets_threshold\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "import json\n",
    "with open('deep_learning_results.json', 'w') as f:\n",
    "    json.dump(results_dict, f, indent=2, default=str)\n",
    "\n",
    "print(\"\\n‚úÖ Results saved to deep_learning_results.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}