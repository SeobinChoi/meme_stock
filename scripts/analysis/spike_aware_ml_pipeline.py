#!/usr/bin/env python3
"""
Spike-Aware Time Series ML Pipeline for Meme Stock Prediction

Reframed approach:
1. Primary: Spike classification (P90 threshold on log1p scale)
2. Secondary: Distribution-aware regression (Poisson/Tweedie)
3. Strengthened baselines: SeasonalMean + Weighted-MA ensemble
4. Robust metrics: PR-AUC, Recall@K, sMASE, SMAPE, RMSLE
5. Statistical validation: Diebold-Mariano tests
"""

import pandas as pd
import numpy as np
from pathlib import Path
import warnings
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import json

# ML and stats
from sklearn.model_selection import TimeSeriesSplit
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder
from sklearn.linear_model import ElasticNet
from sklearn.ensemble import RandomForestRegressor
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.metrics import (mean_absolute_error, mean_squared_error, 
                           precision_recall_curve, average_precision_score,
                           matthews_corrcoef, recall_score, precision_score)
import lightgbm as lgb

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.dates as mdates
from matplotlib.patches import Rectangle

# Statistical tests
from scipy import stats

warnings.filterwarnings('ignore')

DATA_DIR = Path(__file__).resolve().parents[1] / "data"
MODEL_DIR = Path(__file__).resolve().parents[1] / "models" / "spike_aware_production"

# Create output directories
for subdir in ['data', 'models', 'predictions', 'metrics', 'plots', 'reports']:
    (MODEL_DIR / subdir).mkdir(parents=True, exist_ok=True)


class SpikeAwareFeatureEngineer(BaseEstimator, TransformerMixin):
    """Create spike-aware features without data leakage."""
    
    def __init__(self, lag_days=[1, 3, 7, 14, 30], rolling_windows=[7, 14, 30]):
        self.lag_days = lag_days
        self.rolling_windows = rolling_windows
        self.feature_groups = {
            'lag': [],
            'rolling': [],
            'time': [],
            'momentum': [],
            'cross_ticker': [],
            'burst': []
        }
        
    def fit(self, X, y=None):
        return self
    
    def transform(self, X):
        X_feat = X.copy()
        
        # Ensure sorted by ticker and date
        X_feat = X_feat.sort_values(['ticker', 'date']).reset_index(drop=True)
        
        # Log transform for stability
        X_feat['log1p_mentions'] = np.log1p(X_feat['mentions'])
        
        # Time-based features (stronger calendar effects)
        X_feat['day_of_week'] = X_feat['date'].dt.dayofweek
        X_feat['month'] = X_feat['date'].dt.month
        X_feat['quarter'] = X_feat['date'].dt.quarter
        X_feat['is_weekend'] = (X_feat['day_of_week'] >= 5).astype(int)
        X_feat['is_monday'] = (X_feat['day_of_week'] == 0).astype(int)
        X_feat['is_friday'] = (X_feat['day_of_week'] == 4).astype(int)
        X_feat['day_of_month'] = X_feat['date'].dt.day
        X_feat['is_month_end'] = X_feat['date'].dt.is_month_end.astype(int)
        X_feat['week_of_year'] = X_feat['date'].dt.isocalendar().week
        
        self.feature_groups['time'] = ['day_of_week', 'month', 'quarter', 
                                      'is_weekend', 'is_monday', 'is_friday',
                                      'day_of_month', 'is_month_end', 'week_of_year']\n        \n        # Lag features on log scale (with proper shift)\n        for lag in self.lag_days:\n            col_name = f'log_mentions_lag_{lag}'\n            X_feat[col_name] = X_feat.groupby('ticker')['log1p_mentions'].shift(lag)\n            self.feature_groups['lag'].append(col_name)\n        \n        # Rolling features on log scale (shifted to prevent leakage)\n        for window in self.rolling_windows:\n            # EMA for surprise calculation\n            col_name = f'ema_{window}'\n            X_feat[col_name] = X_feat.groupby('ticker')['log1p_mentions'].transform(\n                lambda x: x.shift(1).ewm(span=window).mean()\n            )\n            self.feature_groups['rolling'].append(col_name)\n            \n            # Rolling mean\n            col_name = f'log_mentions_ma_{window}'\n            X_feat[col_name] = X_feat.groupby('ticker')['log1p_mentions'].transform(\n                lambda x: x.shift(1).rolling(window=window, min_periods=1).mean()\n            )\n            self.feature_groups['rolling'].append(col_name)\n            \n            # Rolling std\n            col_name = f'log_mentions_std_{window}'\n            X_feat[col_name] = X_feat.groupby('ticker')['log1p_mentions'].transform(\n                lambda x: x.shift(1).rolling(window=window, min_periods=1).std()\n            )\n            self.feature_groups['rolling'].append(col_name)\n            \n            # Rolling quantiles\n            for q in [0.25, 0.75, 0.9]:\n                col_name = f'log_mentions_q{int(q*100)}_{window}'\n                X_feat[col_name] = X_feat.groupby('ticker')['log1p_mentions'].transform(\n                    lambda x: x.shift(1).rolling(window=window, min_periods=1).quantile(q)\n                )\n                self.feature_groups['rolling'].append(col_name)\n        \n        # Burst/Surprise features\n        # Surprise relative to 7-day EMA\n        X_feat['surprise_7'] = (X_feat.groupby('ticker')['log1p_mentions'].shift(1) - \n                               X_feat['ema_7'])\n        \n        # Historical spike indicators\n        X_feat['was_spike_1d'] = (X_feat['surprise_7'] > \n                                 X_feat.groupby('ticker')['surprise_7'].transform(\n                                     lambda x: x.shift(1).rolling(30, min_periods=1).quantile(0.95)\n                                 )).astype(int)\n        \n        # Momentum (velocity/acceleration on log scale)\n        X_feat['log_velocity_1d'] = X_feat.groupby('ticker')['log1p_mentions'].diff(1)\n        X_feat['log_velocity_7d'] = X_feat.groupby('ticker')['log1p_mentions'].diff(7)\n        X_feat['log_acceleration'] = X_feat.groupby('ticker')['log_velocity_1d'].diff(1)\n        \n        # Volatility measures\n        X_feat['volatility_7d'] = X_feat.groupby('ticker')['log_velocity_1d'].transform(\n            lambda x: x.shift(1).rolling(7, min_periods=1).std()\n        )\n        X_feat['volatility_30d'] = X_feat.groupby('ticker')['log_velocity_1d'].transform(\n            lambda x: x.shift(1).rolling(30, min_periods=1).std()\n        )\n        \n        self.feature_groups['burst'] = ['surprise_7', 'was_spike_1d']\n        self.feature_groups['momentum'] = ['log_velocity_1d', 'log_velocity_7d', \n                                         'log_acceleration', 'volatility_7d', 'volatility_30d']\n        \n        # Cross-ticker features (leave-one-out market sentiment)\n        market_log_total = X_feat.groupby('date')['log1p_mentions'].transform('sum')\n        X_feat['market_sentiment_ex_ticker'] = (market_log_total - X_feat['log1p_mentions']).shift(1)\n        X_feat['market_sentiment_lag1'] = market_log_total.shift(1)\n        \n        # Market volatility\n        X_feat['market_volatility'] = X_feat.groupby('date')['log1p_mentions'].transform('std').shift(1)\n        \n        self.feature_groups['cross_ticker'] = ['market_sentiment_ex_ticker', \n                                             'market_sentiment_lag1', 'market_volatility']\n        \n        # Ticker ID for global models\n        le = LabelEncoder()\n        X_feat['ticker_id'] = le.fit_transform(X_feat['ticker'])\n        \n        # Fill NaN values\n        numeric_cols = X_feat.select_dtypes(include=[np.number]).columns\n        X_feat[numeric_cols] = X_feat[numeric_cols].fillna(0)\n        \n        return X_feat


def create_spike_labels(df: pd.DataFrame, method='p90') -> pd.DataFrame:
    """Create spike labels for classification."""\n    df_labeled = df.copy()\n    \n    if method == 'p90':\n        # Per-ticker 90th percentile on log scale\n        df_labeled['spike_threshold'] = df_labeled.groupby('ticker')['log1p_mentions'].transform(\n            lambda x: x.quantile(0.9)\n        )\n        df_labeled['is_spike'] = (df_labeled['log1p_mentions'] > df_labeled['spike_threshold']).astype(int)\n        \n    elif method == 'zscore':\n        # Z-score > 2 within ticker\n        df_labeled['log_mean'] = df_labeled.groupby('ticker')['log1p_mentions'].transform('mean')\n        df_labeled['log_std'] = df_labeled.groupby('ticker')['log1p_mentions'].transform('std')\n        df_labeled['zscore'] = (df_labeled['log1p_mentions'] - df_labeled['log_mean']) / (df_labeled['log_std'] + 1e-8)\n        df_labeled['is_spike'] = (df_labeled['zscore'] > 2).astype(int)\n    \n    # Future spike (prediction target)\n    df_labeled['spike_next'] = df_labeled.groupby('ticker')['is_spike'].shift(-1)\n    \n    # Drop last day per ticker (no future target)\n    df_labeled = df_labeled.dropna(subset=['spike_next'])\n    df_labeled['spike_next'] = df_labeled['spike_next'].astype(int)\n    \n    print(f\"   Spike rate: {df_labeled['spike_next'].mean():.3f} ({df_labeled['spike_next'].sum()} / {len(df_labeled)})\")\n    \n    return df_labeled


class EnhancedBaselines:\n    \"\"\"Enhanced baseline models with seasonal patterns.\"\"\"\n    \n    @staticmethod\n    def seasonal_mean(train_data, test_dates, ticker):\n        \"\"\"Weekday-specific mean.\"\"\"\n        train_data = train_data[train_data['ticker'] == ticker]\n        \n        # Calculate weekday means\n        weekday_means = train_data.groupby('day_of_week')['log1p_mentions'].mean()\n        \n        # Map to test dates\n        test_df = pd.DataFrame({'date': test_dates})\n        test_df['day_of_week'] = test_df['date'].dt.dayofweek\n        predictions = test_df['day_of_week'].map(weekday_means).fillna(weekday_means.mean())\n        \n        return predictions.values\n    \n    @staticmethod\n    def weighted_ma_ensemble(train_data, test_data, ticker, windows=[3, 7, 14, 30]):\n        \"\"\"Weighted ensemble of MA models based on recent performance.\"\"\"\n        ticker_train = train_data[train_data['ticker'] == ticker]\n        ticker_test = test_data[test_data['ticker'] == ticker]\n        \n        if len(ticker_train) < max(windows):\n            return np.full(len(ticker_test), ticker_train['log1p_mentions'].mean())\n        \n        # Calculate MA predictions\n        ma_preds = {}\n        for window in windows:\n            if len(ticker_train) >= window:\n                ma_value = ticker_train['log1p_mentions'].iloc[-window:].mean()\n                ma_preds[window] = ma_value\n        \n        if not ma_preds:\n            return np.full(len(ticker_test), ticker_train['log1p_mentions'].mean())\n        \n        # Calculate weights based on recent performance (last 30 days)\n        weights = {}\n        recent_data = ticker_train.iloc[-min(60, len(ticker_train)):]\n        \n        for window in ma_preds.keys():\n            if len(recent_data) > window + 7:  # Need validation period\n                # Simulate MA predictions on recent data\n                errors = []\n                for i in range(window, len(recent_data) - 1):\n                    ma_pred = recent_data['log1p_mentions'].iloc[i-window:i].mean()\n                    actual = recent_data['log1p_mentions'].iloc[i+1]\n                    errors.append(abs(ma_pred - actual))\n                \n                if errors:\n                    mae = np.mean(errors)\n                    weights[window] = 1.0 / (mae + 1e-8)  # Inverse weight\n                else:\n                    weights[window] = 1.0\n            else:\n                weights[window] = 1.0\n        \n        # Normalize weights\n        total_weight = sum(weights.values())\n        if total_weight > 0:\n            weights = {w: weights[w] / total_weight for w in weights}\n        else:\n            weights = {w: 1.0 / len(weights) for w in weights}\n        \n        # Weighted prediction\n        ensemble_pred = sum(ma_preds[w] * weights[w] for w in ma_preds)\n        \n        return np.full(len(ticker_test), ensemble_pred)


def calculate_robust_metrics(y_true, y_pred, y_train=None, task='regression'):\n    \"\"\"Calculate robust metrics for both classification and regression.\"\"\"\n    \n    metrics = {}\n    \n    if task == 'classification':\n        # Classification metrics\n        if len(np.unique(y_true)) > 1:  # Avoid issues with constant labels\n            metrics['pr_auc'] = average_precision_score(y_true, y_pred)\n            \n            # Binary predictions for other metrics\n            y_pred_binary = (y_pred > 0.5).astype(int)\n            metrics['mcc'] = matthews_corrcoef(y_true, y_pred_binary)\n            metrics['precision'] = precision_score(y_true, y_pred_binary, zero_division=0)\n            metrics['recall'] = recall_score(y_true, y_pred_binary, zero_division=0)\n            \n            # Recall@K (top K% of predictions)\n            for k in [0.05, 0.1, 0.2]:\n                threshold = np.quantile(y_pred, 1 - k)\n                top_k_mask = y_pred >= threshold\n                if top_k_mask.sum() > 0:\n                    recall_at_k = y_true[top_k_mask].mean()\n                    metrics[f'recall_at_{int(k*100)}pct'] = recall_at_k\n                else:\n                    metrics[f'recall_at_{int(k*100)}pct'] = 0.0\n        else:\n            metrics.update({\n                'pr_auc': 0.0, 'mcc': 0.0, 'precision': 0.0, 'recall': 0.0,\n                'recall_at_5pct': 0.0, 'recall_at_10pct': 0.0, 'recall_at_20pct': 0.0\n            })\n    \n    elif task == 'regression':\n        # Standard metrics\n        metrics['mae'] = mean_absolute_error(y_true, y_pred)\n        metrics['rmse'] = np.sqrt(mean_squared_error(y_true, y_pred))\n        \n        # RMSLE (on log scale, so just RMSE of log values)\n        metrics['rmsle'] = metrics['rmse']  # Already on log scale\n        \n        # SMAPE (Symmetric Mean Absolute Percentage Error)\n        epsilon = 1e-8\n        smape = 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + epsilon))\n        metrics['smape'] = smape\n        \n        # sMASE (seasonal MASE with m=7)\n        if y_train is not None and len(y_train) > 7:\n            # Seasonal naive errors (7-day lag)\n            seasonal_errors = np.abs(y_train.iloc[7:] - y_train.iloc[:-7].values)\n            mae_seasonal_naive = seasonal_errors.mean()\n            \n            if mae_seasonal_naive > 0:\n                smase = metrics['mae'] / mae_seasonal_naive\n                metrics['smase'] = smase\n            else:\n                metrics['smase'] = np.inf\n        else:\n            metrics['smase'] = np.inf\n    \n    return metrics


def diebold_mariano_test(errors1, errors2):\n    \"\"\"Diebold-Mariano test for model comparison.\"\"\"\n    d = errors1**2 - errors2**2\n    d_mean = d.mean()\n    \n    if len(d) < 2 or d.var(ddof=1) == 0:\n        return np.nan, np.nan\n    \n    dm_stat = d_mean / np.sqrt(d.var(ddof=1) / len(d))\n    p_value = 2 * (1 - stats.norm.cdf(abs(dm_stat)))\n    \n    return dm_stat, p_value


def load_and_prepare_data() -> pd.DataFrame:\n    \"\"\"Load ML data and add spike labels.\"\"\"\n    print(\"📊 Loading and preparing spike-aware data...\")\n    \n    # Load the most comprehensive dataset\n    data_files = list((DATA_DIR / 'processed' / 'reddit' / 'ml').glob('reddit_mentions_full_2021_2023_*.csv'))\n    if not data_files:\n        raise FileNotFoundError(\"No ML dataset found. Run process_archive_reddit_data_ml.py first.\")\n    \n    latest_file = max(data_files, key=lambda x: x.stat().st_mtime)\n    df = pd.read_csv(latest_file)\n    \n    # Parse date\n    df['date'] = pd.to_datetime(df['date'])\n    \n    # Create complete date range for all tickers (same as before)\n    date_range = pd.date_range(start=df['date'].min(), end=df['date'].max(), freq='D')\n    tickers = df['ticker'].unique()\n    \n    calendar_grid = []\n    for ticker in tickers:\n        for date in date_range:\n            calendar_grid.append({'ticker': ticker, 'date': date})\n    \n    calendar_df = pd.DataFrame(calendar_grid)\n    df_complete = calendar_df.merge(df[['date', 'ticker', 'mentions', 'ticker_type']], \n                                   on=['date', 'ticker'], how='left')\n    \n    # Fill missing ticker_type\n    ticker_type_map = df[['ticker', 'ticker_type']].drop_duplicates().set_index('ticker')['ticker_type'].to_dict()\n    df_complete['ticker_type'] = df_complete['ticker'].map(ticker_type_map)\n    \n    # Handle missing values\n    df_complete = df_complete.sort_values(['ticker', 'date'])\n    df_complete['mentions'] = df_complete.groupby('ticker')['mentions'].transform(\n        lambda x: x.fillna(method='ffill', limit=3)\n    )\n    \n    # Drop remaining NaN and filter short tickers\n    df_complete = df_complete.dropna(subset=['mentions'])\n    ticker_counts = df_complete.groupby('ticker').size()\n    valid_tickers = ticker_counts[ticker_counts >= 60].index\n    df_final = df_complete[df_complete['ticker'].isin(valid_tickers)].copy()\n    \n    print(f\"   Final dataset: {len(df_final)} rows, {len(valid_tickers)} tickers\")\n    print(f\"   Date range: {df_final['date'].min()} to {df_final['date'].max()}\")\n    \n    return df_final


def train_spike_aware_models(df: pd.DataFrame) -> Dict:\n    \"\"\"Train both classification and regression models.\"\"\"\n    \n    print(\"\\n🚀 Training spike-aware models...\")\n    \n    # Create features\n    feature_engineer = SpikeAwareFeatureEngineer()\n    df_features = feature_engineer.fit_transform(df)\n    \n    # Create spike labels\n    df_labeled = create_spike_labels(df_features, method='p90')\n    \n    # Define feature columns\n    all_features = []\n    for group_features in feature_engineer.feature_groups.values():\n        all_features.extend(group_features)\n    all_features.append('ticker_id')  # Add ticker ID\n    \n    # Create expanding window splits\n    splits = create_expanding_splits(df_labeled)\n    \n    results = {\n        'splits': splits,\n        'classification': {},\n        'regression': {},\n        'baselines': {},\n        'feature_importance': {}\n    }\n    \n    tickers = df_labeled['ticker'].unique()\n    \n    # Models to test\n    models_config = {\n        'seasonal_mean': {'type': 'baseline'},\n        'weighted_ma': {'type': 'baseline'},\n        'lgb_classifier': {'type': 'classification'},\n        'lgb_poisson': {'type': 'regression'},\n        'lgb_quantile': {'type': 'regression'}\n    }\n    \n    print(f\"   Testing {len(models_config)} models on {len(tickers)} tickers\")\n    print(f\"   Using {len(splits)} expanding window splits\")\n    \n    # Initialize results structure\n    for model_name in models_config:\n        if models_config[model_name]['type'] == 'classification':\n            results['classification'][model_name] = {}\n            for ticker in tickers:\n                results['classification'][model_name][ticker] = []\n        elif models_config[model_name]['type'] == 'regression':\n            results['regression'][model_name] = {}\n            for ticker in tickers:\n                results['regression'][model_name][ticker] = []\n        else:\n            results['baselines'][model_name] = {}\n            for ticker in tickers:\n                results['baselines'][model_name][ticker] = []\n    \n    # Train models for each split\n    for split_idx, split_info in enumerate(splits[:5]):  # Limit to 5 splits for demo\n        print(f\"\\n   Processing split {split_idx + 1}/{min(5, len(splits))}\")\n        \n        # Create train/test split\n        train_mask = df_labeled['date'] <= split_info['train_end']\n        test_mask = (df_labeled['date'] >= split_info['val_start']) & \\\n                   (df_labeled['date'] <= split_info['val_end'])\n        \n        if not train_mask.any() or not test_mask.any():\n            continue\n        \n        train_data = df_labeled[train_mask]\n        test_data = df_labeled[test_mask]\n        \n        if len(train_data) < 100:  # Minimum training size\n            continue\n        \n        # Train global models (all tickers together)\n        X_train = train_data[all_features]\n        X_test = test_data[all_features]\n        y_class_train = train_data['spike_next']\n        y_reg_train = train_data['log1p_mentions']\n        \n        # Handle infinite/NaN values\n        X_train = X_train.replace([np.inf, -np.inf], 0).fillna(0)\n        X_test = X_test.replace([np.inf, -np.inf], 0).fillna(0)\n        \n        try:\n            # Train LightGBM Classifier for spikes\n            pos_weight = (len(y_class_train) - y_class_train.sum()) / y_class_train.sum() if y_class_train.sum() > 0 else 1.0\n            \n            lgb_clf = lgb.LGBMClassifier(\n                objective='binary',\n                num_leaves=31,\n                learning_rate=0.05,\n                n_estimators=200,\n                subsample=0.8,\n                colsample_bytree=0.8,\n                scale_pos_weight=pos_weight,\n                random_state=42,\n                verbosity=-1\n            )\n            lgb_clf.fit(X_train, y_class_train)\n            clf_preds = lgb_clf.predict_proba(X_test)[:, 1]\n            \n            # Train LightGBM Poisson for regression\n            lgb_poisson = lgb.LGBMRegressor(\n                objective='poisson',\n                num_leaves=31,\n                learning_rate=0.05,\n                n_estimators=200,\n                subsample=0.8,\n                colsample_bytree=0.8,\n                min_data_in_leaf=20,\n                random_state=42,\n                verbosity=-1\n            )\n            lgb_poisson.fit(X_train, y_reg_train)\n            poisson_preds = lgb_poisson.predict(X_test)\n            \n            # Train LightGBM Quantile for robust regression\n            lgb_quantile = lgb.LGBMRegressor(\n                objective='quantile',\n                alpha=0.5,  # Median\n                num_leaves=31,\n                learning_rate=0.05,\n                n_estimators=200,\n                subsample=0.8,\n                colsample_bytree=0.8,\n                min_data_in_leaf=20,\n                random_state=42,\n                verbosity=-1\n            )\n            lgb_quantile.fit(X_train, y_reg_train)\n            quantile_preds = lgb_quantile.predict(X_test)\n            \n        except Exception as e:\n            print(f\"      Error training models: {e}\")\n            continue\n        \n        # Evaluate per ticker\n        for ticker in tickers:\n            ticker_test = test_data[test_data['ticker'] == ticker]\n            ticker_train = train_data[train_data['ticker'] == ticker]\n            \n            if len(ticker_test) == 0 or len(ticker_train) == 0:\n                continue\n            \n            # Get predictions for this ticker\n            ticker_mask = test_data['ticker'] == ticker\n            \n            # Classification evaluation\n            if ticker_mask.sum() > 0:\n                ticker_clf_preds = clf_preds[ticker_mask]\n                ticker_class_true = test_data[ticker_mask]['spike_next'].values\n                \n                clf_metrics = calculate_robust_metrics(\n                    ticker_class_true, ticker_clf_preds, task='classification'\n                )\n                clf_metrics['split_idx'] = split_idx\n                results['classification']['lgb_classifier'][ticker].append(clf_metrics)\n                \n                # Regression evaluation\n                ticker_reg_true = test_data[ticker_mask]['log1p_mentions'].values\n                \n                # Poisson predictions\n                ticker_poisson_preds = poisson_preds[ticker_mask]\n                poisson_metrics = calculate_robust_metrics(\n                    ticker_reg_true, ticker_poisson_preds, \n                    ticker_train['log1p_mentions'], task='regression'\n                )\n                poisson_metrics['split_idx'] = split_idx\n                results['regression']['lgb_poisson'][ticker].append(poisson_metrics)\n                \n                # Quantile predictions\n                ticker_quantile_preds = quantile_preds[ticker_mask]\n                quantile_metrics = calculate_robust_metrics(\n                    ticker_reg_true, ticker_quantile_preds,\n                    ticker_train['log1p_mentions'], task='regression'\n                )\n                quantile_metrics['split_idx'] = split_idx\n                results['regression']['lgb_quantile'][ticker].append(quantile_metrics)\n                \n                # Baseline predictions\n                test_dates = ticker_test['date']\n                \n                # SeasonalMean baseline\n                seasonal_preds = EnhancedBaselines.seasonal_mean(train_data, test_dates, ticker)\n                seasonal_metrics = calculate_robust_metrics(\n                    ticker_reg_true, seasonal_preds,\n                    ticker_train['log1p_mentions'], task='regression'\n                )\n                seasonal_metrics['split_idx'] = split_idx\n                results['baselines']['seasonal_mean'][ticker].append(seasonal_metrics)\n                \n                # Weighted MA ensemble\n                ma_preds = EnhancedBaselines.weighted_ma_ensemble(train_data, test_data, ticker)\n                ma_metrics = calculate_robust_metrics(\n                    ticker_reg_true, ma_preds,\n                    ticker_train['log1p_mentions'], task='regression'\n                )\n                ma_metrics['split_idx'] = split_idx\n                results['baselines']['weighted_ma'][ticker].append(ma_metrics)\n    \n    print(f\"\\n✅ Spike-aware model training complete!\")\n    return results


def create_expanding_splits(df: pd.DataFrame, \n                           start_date='2021-03-01',\n                           val_months=3,\n                           step_months=1) -> List[Tuple]:\n    \"\"\"Create expanding window splits.\"\"\"\n    \n    splits = []\n    current_date = pd.to_datetime(start_date)\n    end_date = df['date'].max()\n    \n    while current_date + pd.DateOffset(months=val_months) <= end_date:\n        train_end = current_date\n        val_start = current_date\n        val_end = current_date + pd.DateOffset(months=val_months)\n        \n        if val_end > end_date:\n            break\n            \n        splits.append({\n            'train_end': train_end,\n            'val_start': val_start, \n            'val_end': val_end\n        })\n        \n        current_date += pd.DateOffset(months=step_months)\n    \n    print(f\"   Created {len(splits)} expanding window splits\")\n    return splits


def create_spike_visualizations(results: Dict, df: pd.DataFrame):\n    \"\"\"Create spike-aware visualizations.\"\"\"\n    \n    print(\"\\n📊 Creating spike-aware visualizations...\")\n    \n    # Get top 3 tickers by activity\n    top_tickers = df.groupby('ticker')['mentions'].sum().nlargest(3).index.tolist()\n    \n    for ticker in top_tickers:\n        print(f\"   Creating plots for {ticker}\")\n        \n        fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n        fig.suptitle(f'Spike-Aware Model Performance: {ticker}', fontsize=16, fontweight='bold')\n        \n        # Panel 1: Classification Performance (PR Curve)\n        ax1 = axes[0, 0]\n        if ticker in results['classification']['lgb_classifier']:\n            clf_results = results['classification']['lgb_classifier'][ticker]\n            if clf_results:\n                pr_aucs = [r['pr_auc'] for r in clf_results if not np.isnan(r['pr_auc'])]\n                recall_5pct = [r['recall_at_5pct'] for r in clf_results if not np.isnan(r['recall_at_5pct'])]\n                \n                if pr_aucs:\n                    ax1.plot(range(len(pr_aucs)), pr_aucs, 'o-', label=f'PR-AUC (avg: {np.mean(pr_aucs):.3f})')\n                if recall_5pct:\n                    ax1.plot(range(len(recall_5pct)), recall_5pct, 's-', \n                           label=f'Recall@5% (avg: {np.mean(recall_5pct):.3f})', alpha=0.7)\n                \n                ax1.axhline(y=0.65, color='red', linestyle='--', alpha=0.5, label='Target PR-AUC: 0.65')\n                ax1.axhline(y=0.60, color='orange', linestyle='--', alpha=0.5, label='Target Recall@5%: 0.60')\n        \n        ax1.set_title('Spike Classification Performance')\n        ax1.set_xlabel('CV Fold')\n        ax1.set_ylabel('Score')\n        ax1.legend()\n        ax1.grid(True, alpha=0.3)\n        \n        # Panel 2: Regression Performance (sMASE)\n        ax2 = axes[0, 1]\n        models_to_plot = ['seasonal_mean', 'weighted_ma', 'lgb_poisson', 'lgb_quantile']\n        colors = ['red', 'blue', 'green', 'purple']\n        \n        for i, model in enumerate(models_to_plot):\n            smase_values = []\n            \n            if model in ['seasonal_mean', 'weighted_ma'] and ticker in results['baselines'][model]:\n                model_results = results['baselines'][model][ticker]\n                smase_values = [r['smase'] for r in model_results \n                              if not np.isinf(r['smase']) and not np.isnan(r['smase'])]\n            elif model in ['lgb_poisson', 'lgb_quantile'] and ticker in results['regression'][model]:\n                model_results = results['regression'][model][ticker]\n                smase_values = [r['smase'] for r in model_results \n                              if not np.isinf(r['smase']) and not np.isnan(r['smase'])]\n            \n            if smase_values:\n                ax2.plot(range(len(smase_values)), smase_values, 'o-', \n                        color=colors[i], label=f'{model} (avg: {np.mean(smase_values):.2f})', alpha=0.7)\n        \n        ax2.set_title('Regression Performance (sMASE)')\n        ax2.set_xlabel('CV Fold')\n        ax2.set_ylabel('sMASE')\n        ax2.legend()\n        ax2.grid(True, alpha=0.3)\n        \n        # Panel 3: Model Comparison Heatmap\n        ax3 = axes[1, 0]\n        \n        # Create comparison matrix\n        comparison_data = []\n        model_names = []\n        \n        # Baselines\n        for model in ['seasonal_mean', 'weighted_ma']:\n            if ticker in results['baselines'][model] and results['baselines'][model][ticker]:\n                metrics_list = results['baselines'][model][ticker]\n                avg_smase = np.mean([m['smase'] for m in metrics_list \n                                   if not np.isinf(m['smase']) and not np.isnan(m['smase'])])\n                avg_smape = np.mean([m['smape'] for m in metrics_list \n                                   if not np.isnan(m['smape'])])\n                comparison_data.append([avg_smase, avg_smape, 0, 0])  # No classification metrics\n                model_names.append(model)\n        \n        # ML models\n        for model in ['lgb_poisson', 'lgb_quantile']:\n            if ticker in results['regression'][model] and results['regression'][model][ticker]:\n                metrics_list = results['regression'][model][ticker]\n                avg_smase = np.mean([m['smase'] for m in metrics_list \n                                   if not np.isinf(m['smase']) and not np.isnan(m['smase'])])\n                avg_smape = np.mean([m['smape'] for m in metrics_list \n                                   if not np.isnan(m['smape'])])\n                \n                # Get classification metrics for same ticker\n                pr_auc = 0\n                recall_5 = 0\n                if ticker in results['classification']['lgb_classifier']:\n                    clf_metrics = results['classification']['lgb_classifier'][ticker]\n                    if clf_metrics:\n                        pr_auc = np.mean([m['pr_auc'] for m in clf_metrics if not np.isnan(m['pr_auc'])])\n                        recall_5 = np.mean([m['recall_at_5pct'] for m in clf_metrics if not np.isnan(m['recall_at_5pct'])])\n                \n                comparison_data.append([avg_smase, avg_smape, pr_auc, recall_5])\n                model_names.append(model)\n        \n        if comparison_data:\n            comparison_df = pd.DataFrame(comparison_data, \n                                       index=model_names,\n                                       columns=['sMASE', 'SMAPE', 'PR-AUC', 'Recall@5%'])\n            \n            sns.heatmap(comparison_df, annot=True, fmt='.3f', cmap='RdYlGn_r', \n                       ax=ax3, cbar_kws={'label': 'Score'})\n        \n        ax3.set_title('Model Performance Matrix')\n        \n        # Panel 4: Time Series with Spike Highlighting\n        ax4 = axes[1, 1]\n        \n        ticker_data = df[df['ticker'] == ticker].copy().sort_values('date')\n        recent_data = ticker_data.tail(180)  # Last 6 months\n        \n        # Plot mentions\n        ax4.plot(recent_data['date'], recent_data['mentions'], \n                label='Mentions', alpha=0.7, linewidth=1)\n        \n        # Highlight spikes (recreate spike logic)\n        log_mentions = np.log1p(recent_data['mentions'])\n        spike_threshold = log_mentions.quantile(0.9)\n        spike_mask = log_mentions > spike_threshold\n        \n        if spike_mask.any():\n            spike_dates = recent_data[spike_mask]['date']\n            spike_values = recent_data[spike_mask]['mentions']\n            ax4.scatter(spike_dates, spike_values, color='red', s=50, \n                       label=f'Spikes (P90)', alpha=0.8, zorder=5)\n        \n        ax4.set_title('Recent Activity with Spike Detection')\n        ax4.set_ylabel('Mentions')\n        ax4.legend()\n        ax4.grid(True, alpha=0.3)\n        \n        # Format x-axis\n        ax4.tick_params(axis='x', rotation=45)\n        \n        plt.tight_layout()\n        \n        # Save plot\n        plot_path = MODEL_DIR / 'plots' / f'{ticker}_spike_aware_analysis.png'\n        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n        plt.close()\n    \n    print(f\"   Saved spike-aware plots to {MODEL_DIR / 'plots'}\")\n\n\ndef generate_spike_aware_report(results: Dict, df: pd.DataFrame):\n    \"\"\"Generate comprehensive spike-aware report.\"\"\"\n    \n    print(\"\\n📋 Generating spike-aware report...\")\n    \n    report = {\n        'summary': {\n            'generated_at': datetime.now().isoformat(),\n            'approach': 'spike_aware_dual_task',\n            'primary_task': 'spike_classification_p90',\n            'secondary_task': 'distribution_aware_regression',\n            'tickers_analyzed': len(results['classification']['lgb_classifier']),\n            'cv_folds': len([r for ticker_results in results['classification']['lgb_classifier'].values() \n                           for r in ticker_results])\n        },\n        'classification_performance': {},\n        'regression_performance': {},\n        'baseline_comparison': {},\n        'validation_criteria': {},\n        'recommendations': []\n    }\n    \n    # Classification performance summary\n    all_pr_aucs = []\n    all_recall_5pct = []\n    \n    for ticker_results in results['classification']['lgb_classifier'].values():\n        for metrics in ticker_results:\n            if not np.isnan(metrics['pr_auc']):\n                all_pr_aucs.append(metrics['pr_auc'])\n            if not np.isnan(metrics['recall_at_5pct']):\n                all_recall_5pct.append(metrics['recall_at_5pct'])\n    \n    if all_pr_aucs:\n        report['classification_performance'] = {\n            'avg_pr_auc': float(np.mean(all_pr_aucs)),\n            'std_pr_auc': float(np.std(all_pr_aucs)),\n            'avg_recall_at_5pct': float(np.mean(all_recall_5pct)) if all_recall_5pct else 0.0,\n            'meets_pr_auc_threshold': np.mean(all_pr_aucs) >= 0.65,\n            'meets_recall_threshold': np.mean(all_recall_5pct) >= 0.60 if all_recall_5pct else False\n        }\n    \n    # Regression performance comparison\n    model_performance = {}\n    \n    for model_type, model_dict in [('baselines', results['baselines']), \n                                 ('regression', results['regression'])]:\n        for model_name, ticker_dict in model_dict.items():\n            smase_values = []\n            smape_values = []\n            \n            for ticker_results in ticker_dict.values():\n                for metrics in ticker_results:\n                    if not np.isinf(metrics.get('smase', np.inf)) and not np.isnan(metrics.get('smase', np.nan)):\n                        smase_values.append(metrics['smase'])\n                    if not np.isnan(metrics.get('smape', np.nan)):\n                        smape_values.append(metrics['smape'])\n            \n            if smase_values:\n                model_performance[model_name] = {\n                    'avg_smase': float(np.mean(smase_values)),\n                    'avg_smape': float(np.mean(smape_values)) if smape_values else np.nan,\n                    'n_evaluations': len(smase_values)\n                }\n    \n    # Find best models\n    if model_performance:\n        best_model = min(model_performance.items(), key=lambda x: x[1]['avg_smase'])\n        baseline_smase = model_performance.get('seasonal_mean', {}).get('avg_smase', np.inf)\n        \n        report['regression_performance'] = model_performance\n        \n        if baseline_smase != np.inf:\n            improvement = (baseline_smase - best_model[1]['avg_smase']) / baseline_smase * 100\n            \n            report['validation_criteria'] = {\n                'best_model': best_model[0],\n                'best_smase': best_model[1]['avg_smase'],\n                'baseline_smase': baseline_smase,\n                'smase_improvement_pct': improvement,\n                'meets_improvement_threshold': improvement >= 5.0\n            }\n    \n    # Recommendations based on results\n    if report.get('classification_performance', {}).get('meets_pr_auc_threshold', False):\n        report['recommendations'].append(\"✅ Spike classification meets PR-AUC threshold (≥0.65)\")\n    else:\n        report['recommendations'].append(\"❌ Spike classification below target. Consider feature engineering or class balancing.\")\n    \n    if report.get('validation_criteria', {}).get('meets_improvement_threshold', False):\n        report['recommendations'].append(\"✅ Regression model meets improvement threshold (≥5%)\")\n    else:\n        report['recommendations'].append(\"❌ Regression improvement insufficient. Try ensemble or different loss functions.\")\n    \n    # Overall assessment\n    classification_good = report.get('classification_performance', {}).get('meets_pr_auc_threshold', False)\n    regression_good = report.get('validation_criteria', {}).get('meets_improvement_threshold', False)\n    \n    if classification_good and regression_good:\n        report['recommendations'].append(\"🎯 Both tasks show promising results. Ready for production testing.\")\n    elif classification_good:\n        report['recommendations'].append(\"🔄 Focus on spike detection task - shows better performance than count regression.\")\n    else:\n        report['recommendations'].append(\"⚠️  Consider pivot to ranking/top-K approach as suggested.\")\n    \n    # Save reports\n    report_path = MODEL_DIR / 'reports' / f'spike_aware_report_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\n    with open(report_path, 'w') as f:\n        json.dump(report, f, indent=2)\n    \n    # Text summary\n    summary_path = MODEL_DIR / 'reports' / f'spike_summary_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.txt'\n    with open(summary_path, 'w') as f:\n        f.write(\"🚀 SPIKE-AWARE MEME STOCK PREDICTION - EVALUATION REPORT\\n\")\n        f.write(\"=\" * 70 + \"\\n\\n\")\n        \n        f.write(f\"📊 Approach Summary:\\n\")\n        f.write(f\"   - Primary: Spike Classification (P90 threshold)\\n\")\n        f.write(f\"   - Secondary: Distribution-aware Regression (Poisson/Quantile)\\n\")\n        f.write(f\"   - Tickers: {report['summary']['tickers_analyzed']}\\n\\n\")\n        \n        if 'classification_performance' in report:\n            cp = report['classification_performance']\n            f.write(f\"🎯 Classification Results:\\n\")\n            f.write(f\"   - Average PR-AUC: {cp['avg_pr_auc']:.3f} (target: ≥0.65)\\n\")\n            f.write(f\"   - Average Recall@5%: {cp['avg_recall_at_5pct']:.3f} (target: ≥0.60)\\n\")\n            f.write(f\"   - Meets criteria: {cp['meets_pr_auc_threshold'] and cp['meets_recall_threshold']}\\n\\n\")\n        \n        if 'validation_criteria' in report:\n            vc = report['validation_criteria']\n            f.write(f\"📈 Regression Results:\\n\")\n            f.write(f\"   - Best model: {vc['best_model']}\\n\")\n            f.write(f\"   - sMASE improvement: {vc['smase_improvement_pct']:.1f}% (target: ≥5%)\\n\")\n            f.write(f\"   - Meets criteria: {vc['meets_improvement_threshold']}\\n\\n\")\n        \n        f.write(f\"💡 Recommendations:\\n\")\n        for rec in report['recommendations']:\n            f.write(f\"   {rec}\\n\")\n    \n    print(f\"   Reports saved to {MODEL_DIR / 'reports'}\")\n    return report\n\n\ndef main():\n    \"\"\"Main spike-aware pipeline execution.\"\"\"\n    \n    print(\"🚀 Starting Spike-Aware Time Series ML Pipeline\")\n    print(\"=\" * 70)\n    \n    # Load and prepare data\n    df = load_and_prepare_data()\n    \n    # Train spike-aware models\n    results = train_spike_aware_models(df)\n    \n    # Create visualizations\n    create_spike_visualizations(results, df)\n    \n    # Generate report\n    report = generate_spike_aware_report(results, df)\n    \n    print(\"\\n🎯 Spike-aware pipeline completed successfully!\")\n    print(f\"📁 All outputs saved to: {MODEL_DIR}\")\n    \n    # Print quick summary\n    if 'classification_performance' in report:\n        cp = report['classification_performance']\n        print(f\"\\n📊 Quick Results:\")\n        print(f\"   Spike Classification PR-AUC: {cp['avg_pr_auc']:.3f}\")\n        print(f\"   Meets classification criteria: {cp['meets_pr_auc_threshold']}\")\n    \n    if 'validation_criteria' in report:\n        vc = report['validation_criteria']\n        print(f\"   Best regression model: {vc['best_model']}\")\n        print(f\"   sMASE improvement: {vc['smase_improvement_pct']:.1f}%\")\n        print(f\"   Meets regression criteria: {vc['meets_improvement_threshold']}\")\n\n\nif __name__ == '__main__':\n    main()