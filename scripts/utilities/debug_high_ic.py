#!/usr/bin/env python3
"""
Debug High IC Issue
==================
IC 0.07-0.08ÏùÄ ÎπÑÌòÑÏã§Ï†ÅÏúºÎ°ú ÎÜíÏùå. ÏõêÏù∏ Î∂ÑÏÑù:

1. Data Leakage Ï≤¥ÌÅ¨
2. Overfitting ÌôïÏù∏  
3. Feature Í≤ÄÏ¶ù
4. Train/Test Split Î¨∏Ï†ú
5. Ïä§ÏºÄÏùºÎßÅ Ïù¥Ïäà
"""

import pandas as pd
import numpy as np
from scipy.stats import pearsonr, spearmanr
import matplotlib.pyplot as plt
from sklearn.preprocessing import RobustScaler
import lightgbm as lgb
import xgboost as xgb

def debug_high_ic():
    """ÎÜíÏùÄ IC ÏõêÏù∏ Î∂ÑÏÑù"""
    print("üîç DEBUGGING HIGH IC (0.07-0.08)")
    print("=" * 50)
    print("üí° Ï†ïÏÉÅÏ†ÅÏù∏ IC Î≤îÏúÑ: 0.01-0.05 (0.08ÏùÄ ÎπÑÌòÑÏã§Ï†Å)")
    
    # 1. Îç∞Ïù¥ÌÑ∞ Î°úÎìú
    print("\nüìä 1. DATA INSPECTION:")
    train_df = pd.read_csv("data/colab_datasets/tabular_train_20250814_031335.csv")
    test_df = pd.read_csv("data/colab_datasets/tabular_test_20250814_031335.csv")
    
    print(f"   Train shape: {train_df.shape}")
    print(f"   Test shape: {test_df.shape}")
    
    # ÌÉÄÍ≤ü Î∂ÑÌè¨ ÌôïÏù∏
    print(f"\nüìà 2. TARGET DISTRIBUTION:")
    print(f"   Train y1d: mean={train_df['y1d'].mean():.6f}, std={train_df['y1d'].std():.6f}")
    print(f"   Test y1d: mean={test_df['y1d'].mean():.6f}, std={test_df['y1d'].std():.6f}")
    print(f"   Train range: [{train_df['y1d'].min():.4f}, {train_df['y1d'].max():.4f}]")
    print(f"   Test range: [{test_df['y1d'].min():.4f}, {test_df['y1d'].max():.4f}]")
    
    # 3. Feature ÎàÑÏàò Ï≤¥ÌÅ¨
    print(f"\nüö® 3. DATA LEAKAGE CHECK:")
    
    # ÎØ∏Îûò Ï†ïÎ≥¥Í∞Ä Ìè¨Ìï®Îêú Í≤É Í∞ôÏùÄ featureÎì§ Ï≤¥ÌÅ¨
    suspicious_features = [col for col in train_df.columns 
                          if any(word in col.lower() for word in ['future', 'next', 'lead', 'ahead'])]
    print(f"   Suspicious feature names: {suspicious_features}")
    
    # y1dÏôÄ ÎÑàÎ¨¥ ÎÜíÏùÄ ÏÉÅÍ¥ÄÍ¥ÄÍ≥ÑÎ•º Î≥¥Ïù¥Îäî features
    feature_cols = [col for col in train_df.columns if col not in ['y1d', 'date', 'ticker']]
    high_corr_features = []
    
    for feature in feature_cols:
        if train_df[feature].dtype in ['float64', 'int64']:
            corr = train_df[feature].corr(train_df['y1d'])
            if abs(corr) > 0.5:  # 50% Ïù¥ÏÉÅ ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ
                high_corr_features.append((feature, corr))
    
    print(f"   Features with |correlation| > 0.5 with y1d:")
    for feature, corr in sorted(high_corr_features, key=lambda x: abs(x[1]), reverse=True)[:10]:
        print(f"      {feature}: {corr:.4f}")
    
    # 4. Time Series Split Î¨∏Ï†ú Ï≤¥ÌÅ¨
    print(f"\nüìÖ 4. TIME SERIES VALIDATION:")
    
    # ÎÇ†Ïßú Ï†ïÎ≥¥Í∞Ä ÏûàÎäîÏßÄ ÌôïÏù∏
    if 'date' in train_df.columns:
        print("   Date column found - checking chronological order...")
        # Ïã§Ï†ú ÏãúÍ≥ÑÏó¥ Î∂ÑÌï†Ïù¥ ÎêòÏóàÎäîÏßÄ ÌôïÏù∏ÌïòÎ†§Î©¥ ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞ ÌïÑÏöî
    else:
        print("   ‚ö†Ô∏è No date column - cannot verify time series split")
    
    # 5. Í∞ÑÎã®Ìïú Î™®Îç∏Î°ú ÌòÑÏã§Ï†Å IC ÌÖåÏä§Ìä∏
    print(f"\nü§ñ 5. REALISTIC MODEL TEST:")
    
    # Top featuresÎßå ÏÇ¨Ïö©Ìï¥ÏÑú Í∞ÑÎã®Ìïú Î™®Îç∏ ÌÖåÏä§Ìä∏
    top_features = [
        'log_mentions', 'price_ratio_sma20', 'price_ratio_sma10', 
        'returns_3d', 'returns_1d', 'market_sentiment'
    ]
    
    available_features = [f for f in top_features if f in train_df.columns]
    print(f"   Using {len(available_features)} conservative features")
    
    # Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ
    X_train = train_df[available_features].fillna(0).values
    y_train = train_df['y1d'].values
    X_test = test_df[available_features].fillna(0).values  
    y_test = test_df['y1d'].values
    
    # Ïä§ÏºÄÏùºÎßÅ
    scaler = RobustScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # Í∞ÑÎã®Ìïú ÏÑ†Ìòï Î™®Îç∏Î∂ÄÌÑ∞ ÌÖåÏä§Ìä∏
    from sklearn.linear_model import Ridge
    
    ridge_model = Ridge(alpha=1.0)
    ridge_model.fit(X_train_scaled, y_train)
    ridge_pred = ridge_model.predict(X_test_scaled)
    
    ridge_ic, _ = pearsonr(y_test, ridge_pred)
    ridge_hit_rate = np.mean(np.sign(y_test) == np.sign(ridge_pred))
    
    print(f"   Ridge Regression:")
    print(f"      IC: {ridge_ic:.4f}")
    print(f"      Hit Rate: {ridge_hit_rate:.3f}")
    
    # LightGBM with conservative parameters
    lgb_conservative = lgb.LGBMRegressor(
        objective='regression',
        num_leaves=20,  # Îß§Ïö∞ ÏûëÍ≤å
        max_depth=3,    # ÏñïÍ≤å
        learning_rate=0.1,
        n_estimators=50,  # Ï†ÅÍ≤å
        subsample=0.8,
        colsample_bytree=0.8,
        reg_alpha=1.0,  # Í∞ïÌïú Ï†ïÍ∑úÌôî
        reg_lambda=1.0,
        random_state=42,
        verbosity=-1
    )
    
    lgb_conservative.fit(X_train_scaled, y_train)
    lgb_pred = lgb_conservative.predict(X_test_scaled)
    
    lgb_ic, _ = pearsonr(y_test, lgb_pred)
    lgb_hit_rate = np.mean(np.sign(y_test) == np.sign(lgb_pred))
    
    print(f"   Conservative LightGBM:")
    print(f"      IC: {lgb_ic:.4f}")
    print(f"      Hit Rate: {lgb_hit_rate:.3f}")
    
    # 6. Cross-validationÏúºÎ°ú ÏïàÏ†ïÏÑ± Ï≤¥ÌÅ¨
    print(f"\nüîÑ 6. CROSS-VALIDATION STABILITY:")
    
    from sklearn.model_selection import TimeSeriesSplit
    
    tscv = TimeSeriesSplit(n_splits=3)
    cv_ics = []
    
    for train_idx, val_idx in tscv.split(X_train_scaled):
        X_cv_train, X_cv_val = X_train_scaled[train_idx], X_train_scaled[val_idx]
        y_cv_train, y_cv_val = y_train[train_idx], y_train[val_idx]
        
        cv_model = Ridge(alpha=1.0)
        cv_model.fit(X_cv_train, y_cv_train)
        cv_pred = cv_model.predict(X_cv_val)
        
        cv_ic, _ = pearsonr(y_cv_val, cv_pred)
        cv_ics.append(cv_ic)
    
    print(f"   CV IC scores: {[f'{ic:.4f}' for ic in cv_ics]}")
    print(f"   CV IC mean: {np.mean(cv_ics):.4f}")
    print(f"   CV IC std: {np.std(cv_ics):.4f}")
    
    # 7. Feature importance Î∂ÑÏÑù
    print(f"\nüîç 7. FEATURE IMPORTANCE ANALYSIS:")
    
    # Ridge Í≥ÑÏàò ÌôïÏù∏
    feature_importance = dict(zip(available_features, ridge_model.coef_))
    top_important = sorted(feature_importance.items(), key=lambda x: abs(x[1]), reverse=True)
    
    print(f"   Top important features (Ridge coefficients):")
    for feature, coef in top_important:
        print(f"      {feature}: {coef:.4f}")
    
    # 8. ÏòàÏ∏°Í∞í Î∂ÑÌè¨ ÌôïÏù∏
    print(f"\nüìä 8. PREDICTION DISTRIBUTION:")
    print(f"   Ridge predictions: mean={np.mean(ridge_pred):.6f}, std={np.std(ridge_pred):.6f}")
    print(f"   LightGBM predictions: mean={np.mean(lgb_pred):.6f}, std={np.std(lgb_pred):.6f}")
    print(f"   Actual values: mean={np.mean(y_test):.6f}, std={np.std(y_test):.6f}")
    
    # ÏòàÏ∏°Í∞íÏù¥ ÎÑàÎ¨¥ Ïã§Ï†úÍ∞íÍ≥º ÎπÑÏä∑ÌïúÏßÄ Ï≤¥ÌÅ¨ (Ïò§Î≤ÑÌîºÌåÖ Ïã†Ìò∏)
    pred_vs_actual_corr = pearsonr(ridge_pred, y_test)[0]
    print(f"   Prediction vs Actual correlation: {pred_vs_actual_corr:.4f}")
    
    # 9. Í≤∞Î°† Î∞è Í∂åÏû•ÏÇ¨Ìï≠
    print(f"\nüí° 9. CONCLUSIONS & RECOMMENDATIONS:")
    
    # ÌòÑÏã§Ï†ÅÏù∏ IC Î≤îÏúÑÏù∏ÏßÄ Ï≤¥ÌÅ¨
    realistic_ics = [ridge_ic, lgb_ic] + cv_ics
    max_realistic_ic = max(abs(ic) for ic in realistic_ics)
    
    print(f"   Realistic IC range: {min(realistic_ics):.4f} to {max(realistic_ics):.4f}")
    print(f"   Maximum |IC| achieved: {max_realistic_ic:.4f}")
    
    if max_realistic_ic > 0.05:
        print("   ‚ö†Ô∏è Still high IC - possible data issues")
    elif max_realistic_ic > 0.03:
        print("   ‚úÖ High but reasonable IC - good model")
    elif max_realistic_ic > 0.01:
        print("   ‚úÖ Moderate IC - acceptable performance")
    else:
        print("   ‚ùå Low IC - model not effective")
    
    # ÏõêÎûò ÎÜíÏùÄ IC ÏõêÏù∏ Ï∂îÏ†ï
    print(f"\nüéØ LIKELY CAUSES OF HIGH IC (0.07-0.08):")
    if len(high_corr_features) > 0:
        print("   1. ‚ö†Ô∏è High correlation features detected - possible leakage")
    if max_realistic_ic < 0.03:
        print("   2. ‚ö†Ô∏è Conservative models show much lower IC - overfitting")
    print("   3. ‚ö†Ô∏è Complex models (deep trees) may be memorizing patterns")
    print("   4. ‚ö†Ô∏è Small dataset size can lead to unstable high IC")
    
    print(f"\nüìã RECOMMENDATIONS:")
    print("   1. Use conservative model parameters")
    print("   2. Increase regularization (alpha/lambda)")
    print("   3. Reduce model complexity (depth, leaves)")
    print("   4. Cross-validate all results")
    print("   5. Check for data leakage carefully")
    print(f"   6. Report conservative IC (~{max_realistic_ic:.3f}) in paper")

def check_data_leakage_detailed():
    """Îç∞Ïù¥ÌÑ∞ ÎàÑÏàò ÏÉÅÏÑ∏ Ï≤¥ÌÅ¨"""
    print("\nüö® DETAILED DATA LEAKAGE CHECK")
    print("=" * 40)
    
    # Colab Îç∞Ïù¥ÌÑ∞ÏÖã Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ ÌôïÏù∏
    try:
        with open("data/colab_datasets/dataset_metadata_20250814_031335.json", 'r') as f:
            import json
            metadata = json.load(f)
            
        print("üìã Dataset metadata found:")
        if 'feature_info' in metadata:
            print("   Feature info available")
        if 'creation_process' in metadata:
            print(f"   Creation process: {metadata['creation_process']}")
            
    except Exception as e:
        print(f"   No metadata file: {e}")
    
    # ÏõêÎ≥∏ Í≥†Í∏â features Îç∞Ïù¥ÌÑ∞ÏÖãÍ≥º ÎπÑÍµê
    try:
        original_df = pd.read_csv("data/features/advanced_meme_features_dataset.csv", nrows=5)
        colab_df = pd.read_csv("data/colab_datasets/tabular_train_20250814_031335.csv", nrows=5)
        
        print(f"\nüìä Dataset comparison:")
        print(f"   Original features: {original_df.shape[1]}")
        print(f"   Colab features: {colab_df.shape[1]}")
        
        # Í≥µÌÜµ features
        common_features = set(original_df.columns) & set(colab_df.columns)
        print(f"   Common features: {len(common_features)}")
        
    except Exception as e:
        print(f"   Cannot compare datasets: {e}")

if __name__ == "__main__":
    debug_high_ic()
    check_data_leakage_detailed()
    
    print(f"\n‚úÖ Debug analysis completed!")
    print(f"üí° Use conservative IC estimates for paper")

