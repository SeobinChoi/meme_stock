# ğŸš€ Colab A100 GPU ë””ë²„ê¹… ê°€ì´ë“œ

## ğŸ¯ **LSTM ë°ì´í„° ë¡œë”© ë¬¸ì œ í•´ê²°**

### **ë¬¸ì œ 1: ë¬¸ìì—´ ë°ì´í„° íƒ€ì… ì˜¤ë¥˜**
```python
# âŒ ê¸°ì¡´ ì½”ë“œ (ì˜¤ë¥˜ ë°œìƒ)
sequences_data = np.load(f'sequences_{timestamp}.npz')
X_seq = np.vstack(all_sequences)  # ë¬¸ìì—´ í¬í•¨ ì‹œ ì˜¤ë¥˜

# âœ… ìˆ˜ì •ëœ ì½”ë“œ
def prepare_sequence_data_fixed():
    """A100 GPU ìµœì í™”ëœ ì‹œí€€ìŠ¤ ë°ì´í„° ì¤€ë¹„"""
    
    all_sequences = []
    all_targets = []
    all_dates = []
    
    # ë°ì´í„° íƒ€ì… ê°•ì œ ë³€í™˜
    for ticker in metadata['tickers']:
        if f'{ticker}_sequences' in sequences_data:
            sequences = sequences_data[f'{ticker}_sequences']
            targets = sequences_data[f'{ticker}_targets_1d']
            dates = sequences_data[f'{ticker}_dates']
            
            # ë¬¸ìì—´ ì œê±° ë° ìˆ«ìë§Œ ì¶”ì¶œ
            if sequences.dtype == object:
                # ë¬¸ìì—´ì´ í¬í•¨ëœ ê²½ìš° ìˆ«ì ì»¬ëŸ¼ë§Œ ì„ íƒ
                numeric_cols = []
                for i in range(sequences.shape[2]):
                    try:
                        # ê° ì»¬ëŸ¼ì„ floatë¡œ ë³€í™˜ ì‹œë„
                        test_col = sequences[:, :, i].astype(float)
                        numeric_cols.append(i)
                    except:
                        continue
                
                if len(numeric_cols) > 0:
                    sequences = sequences[:, :, numeric_cols].astype(np.float32)
                else:
                    print(f"âš ï¸ Warning: {ticker} has no numeric columns, skipping")
                    continue
            else:
                sequences = sequences.astype(np.float32)
            
            # NaN ê°’ ì²˜ë¦¬
            sequences = np.nan_to_num(sequences, nan=0.0, posinf=0.0, neginf=0.0)
            
            all_sequences.append(sequences)
            all_targets.extend(targets)
            all_dates.extend(dates)
    
    if not all_sequences:
        raise ValueError("âŒ No valid numeric sequences found!")
    
    # A100 ìµœì í™”: float32 ì‚¬ìš©
    X_seq = np.vstack(all_sequences).astype(np.float32)
    y_seq = np.array(all_targets, dtype=np.float32)
    
    print(f"âœ… Sequence data prepared: {X_seq.shape}, dtype: {X_seq.dtype}")
    return X_seq, y_seq, all_dates
```

### **ë¬¸ì œ 2: ë©”ëª¨ë¦¬ ë¶€ì¡± í•´ê²°**
```python
# A100 GPU ë©”ëª¨ë¦¬ ìµœì í™”
import torch

# GPU ë©”ëª¨ë¦¬ ì •ë¦¬
torch.cuda.empty_cache()

# í˜¼í•© ì •ë°€ë„ í›ˆë ¨ (FP16)
from torch.cuda.amp import autocast, GradScaler

# ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì¦ê°€ (A100 ë©”ëª¨ë¦¬ í™œìš©)
BATCH_SIZE = 256  # ê¸°ì¡´ 64ì—ì„œ ì¦ê°€
```

### **ë¬¸ì œ 3: ì°¨ì› ë¶ˆì¼ì¹˜ í•´ê²°**
```python
def validate_sequence_dimensions(X_seq, y_seq):
    """ì‹œí€€ìŠ¤ ì°¨ì› ê²€ì¦"""
    
    print(f"ğŸ” Sequence validation:")
    print(f"   X shape: {X_seq.shape}")
    print(f"   y shape: {y_seq.shape}")
    print(f"   X dtype: {X_seq.dtype}")
    print(f"   y dtype: {y_seq.dtype}")
    
    # ì°¨ì› ê²€ì¦
    if len(X_seq.shape) != 3:
        raise ValueError(f"âŒ Expected 3D array, got {len(X_seq.shape)}D")
    
    if X_seq.shape[0] != len(y_seq):
        raise ValueError(f"âŒ Sample count mismatch: X={X_seq.shape[0]}, y={len(y_seq)}")
    
    # NaN/Inf ê²€ì‚¬
    if np.any(np.isnan(X_seq)) or np.any(np.isinf(X_seq)):
        print("âš ï¸ Warning: NaN/Inf detected in sequences, cleaning...")
        X_seq = np.nan_to_num(X_seq, nan=0.0, posinf=0.0, neginf=0.0)
    
    print("âœ… Sequence validation passed!")
    return X_seq, y_seq
```

## ğŸš€ **A100 GPU ìµœì í™” ì„¤ì •**

### **GPU ì„¤ì •**
```python
# A100 GPU ìµœì í™”
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
if device.type == 'cuda':
    print(f"ğŸš€ Using GPU: {torch.cuda.get_device_name(0)}")
    print(f"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    
    # A100 ìµœì í™” ì„¤ì •
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False
    
    # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
    torch.cuda.set_per_process_memory_fraction(0.95)  # 95% ë©”ëª¨ë¦¬ ì‚¬ìš©
else:
    print("ğŸ’» Using CPU")
```

### **í˜¼í•© ì •ë°€ë„ í›ˆë ¨**
```python
# FP16 í›ˆë ¨ìœ¼ë¡œ A100 ì„±ëŠ¥ ê·¹ëŒ€í™”
scaler = GradScaler()

def train_with_amp(model, train_loader, optimizer, criterion):
    """í˜¼í•© ì •ë°€ë„ í›ˆë ¨"""
    
    model.train()
    total_loss = 0
    
    for batch_X, batch_y in train_loader:
        batch_X, batch_y = batch_X.to(device), batch_y.to(device)
        
        optimizer.zero_grad()
        
        # FP16 forward pass
        with autocast():
            outputs = model(batch_X).squeeze()
            loss = criterion(outputs, batch_y)
        
        # FP16 backward pass
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        
        total_loss += loss.item()
    
    return total_loss / len(train_loader)
```

## ğŸ”§ **ì¦‰ì‹œ ì ìš©í•  ìˆ˜ì •ì‚¬í•­**

### **1. ë°ì´í„° ë¡œë”© ë¶€ë¶„ ìˆ˜ì •**
```python
# ê¸°ì¡´ prepare_sequence_data() í•¨ìˆ˜ë¥¼ ìœ„ì˜ prepare_sequence_data_fixed()ë¡œ êµì²´
```

### **2. LSTM ëª¨ë¸ í›ˆë ¨ ë¶€ë¶„ ìˆ˜ì •**
```python
# ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì¦ê°€
BATCH_SIZE = 256

# ë©”ëª¨ë¦¬ ì •ë¦¬ ì¶”ê°€
torch.cuda.empty_cache()

# ì°¨ì› ê²€ì¦ ì¶”ê°€
X_seq, y_seq = validate_sequence_dimensions(X_seq, y_seq)
```

### **3. ì˜¤ë¥˜ ì²˜ë¦¬ ê°•í™”**
```python
try:
    # ì‹œí€€ìŠ¤ ë°ì´í„° ì¤€ë¹„
    X_seq, y_seq, dates_seq = prepare_sequence_data_fixed()
    
    # ì°¨ì› ê²€ì¦
    X_seq, y_seq = validate_sequence_dimensions(X_seq, y_seq)
    
    print("âœ… Sequence data loaded successfully!")
    
except Exception as e:
    print(f"âŒ Error loading sequence data: {e}")
    print("ğŸ”§ Fallback to tabular models only...")
    
    # ì‹œí€€ìŠ¤ ëª¨ë¸ ë¹„í™œì„±í™”
    USE_SEQUENCE_MODELS = False
```

## ğŸ“Š **ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ**

### **A100 vs T4 ë¹„êµ**
- **ë©”ëª¨ë¦¬**: 40GB vs 16GB (2.5ë°° ì¦ê°€)
- **ë°°ì¹˜ ì‚¬ì´ì¦ˆ**: 256 vs 64 (4ë°° ì¦ê°€)
- **í›ˆë ¨ ì†ë„**: 3-5ë°° í–¥ìƒ
- **ëª¨ë¸ í¬ê¸°**: ë” í° hidden layers ê°€ëŠ¥

### **ê¶Œì¥ ëª¨ë¸ ì„¤ì •**
```python
# A100 ìµœì í™”ëœ ëª¨ë¸ í¬ê¸°
LSTM_HIDDEN_SIZE = 256  # ê¸°ì¡´ 128ì—ì„œ ì¦ê°€
TRANSFORMER_D_MODEL = 256  # ê¸°ì¡´ 128ì—ì„œ ì¦ê°€
MLP_HIDDEN_DIMS = [1024, 512, 256, 128]  # ê¸°ì¡´ë³´ë‹¤ í° ëª¨ë¸
```

## ğŸ¯ **ì„±ê³µ ê¸°ì¤€ ë‹¬ì„± ì „ëµ**

### **IC â‰¥ 0.03 ë‹¬ì„± ë°©ì•ˆ**
1. **ë” í° ëª¨ë¸**: A100 ë©”ëª¨ë¦¬ í™œìš©í•œ ëŒ€í˜• ëª¨ë¸
2. **í˜¼í•© ì •ë°€ë„**: FP16ìœ¼ë¡œ ì•ˆì •ì ì¸ í›ˆë ¨
3. **ë°ì´í„° í’ˆì§ˆ**: ë¬¸ìì—´ ì˜¤ë¥˜ ì™„ì „ ì œê±°
4. **ì•™ìƒë¸”**: ì—¬ëŸ¬ ëª¨ë¸ ì¡°í•©ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ

ì´ ê°€ì´ë“œë¥¼ ë”°ë¼ ìˆ˜ì •í•˜ë©´ A100ì—ì„œ ì•ˆì •ì ìœ¼ë¡œ ì‹¤í–‰ë˜ê³  ì„±ëŠ¥ë„ í¬ê²Œ í–¥ìƒë  ê²ƒì…ë‹ˆë‹¤!
